{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Decision trees\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will test our knowledge of the fundamental concepts of decision trees by implementing a decision tree regression model and analysing its performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this exercise, you should be able to:\n",
    "* Implement a decision tree regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using our agriculture dataset to explore the effect climate change has on crops in Maji Ndogo. As a refresher of our data, examine the data dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float). (DUMMY VARIABLE – the simulation might have created a relationship)\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float). (DUMMY VARIABLE – the simulation might have created a relationship)\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float). (DUMMY VARIABLE)\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float). (DUMMY VARIABLE)\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celsius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float). (DUMMY VARIABLE)\n",
    "\n",
    "- **Crop_type:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the annual yield. (DUMMY VARIABLE – removed)\n",
    "\n",
    "<br>\n",
    "\n",
    "**5. Target variable**\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size or crop type. Multiplying this number by the field size and average crop yield will give the Annual_yield.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our database again, like we did last time. We won't use the weather data so it is commented out.\n",
    "\n",
    "**Important:** Ensure that `data_ingestion.py` file and the `field_data_processor.py` files are stored in the same folder as your notebook, and check that the database file is created correctly, otherwise the data import will fail.\n",
    " \n",
    "[Download files here](https://github.com/Explore-AI/Public-Data/raw/master/Maji_Ndogo/modules.zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 05:06:58,217 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-03-23 05:06:58,217 - data_ingestion - ERROR - An error occurred while querying the database. Error: (sqlite3.OperationalError) no such table: geographic_features\n",
      "[SQL: \n",
      "            SELECT *\n",
      "            FROM geographic_features\n",
      "            LEFT JOIN weather_features USING (Field_ID)\n",
      "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
      "            LEFT JOIN farm_management_features USING (Field_ID)\n",
      "            ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: geographic_features\n[SQL: \n            SELECT *\n            FROM geographic_features\n            LEFT JOIN weather_features USING (Field_ID)\n            LEFT JOIN soil_and_crop_features USING (Field_ID)\n            LEFT JOIN farm_management_features USING (Field_ID)\n            ]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1969\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[0;32m   1970\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[0;32m   1971\u001b[0m         )\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:922\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 922\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: geographic_features",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Ignoring the field data for now.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m field_processor \u001b[38;5;241m=\u001b[39m FieldDataProcessor(config_params)\n\u001b[1;32m---> 33\u001b[0m field_processor\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[0;32m     34\u001b[0m field_df \u001b[38;5;241m=\u001b[39m field_processor\u001b[38;5;241m.\u001b[39mdf\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# We're not going to use the weather data this time, so we'll ignore it.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# weather_processor = WeatherDataProcessor(config_params)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# weather_processor.process()\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# weather_df = weather_processor.weather_df\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\ALX Learning projects\\Regression\\field_data_processor.py:183\u001b[0m, in \u001b[0;36mFieldDataProcessor.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    Applies corrections to the DataFrame based on mappings specified in values_to_rename.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    - The DataFrame with corrections applied to the specified column.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mingest_sql_data()\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrename_columns()\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_corrections()\n",
      "File \u001b[1;32m~\\Documents\\ALX Learning projects\\Regression\\field_data_processor.py:110\u001b[0m, in \u001b[0;36mFieldDataProcessor.ingest_sql_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03mIngests data from a SQL database based on the configured database path and SQL query.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m- A pandas DataFrame containing the data fetched from the database.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m create_db_engine(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m query_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_query)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSucessfully loaded data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\ALX Learning projects\\Regression\\data_ingestion.py:79\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     78\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\Documents\\ALX Learning projects\\Regression\\data_ingestion.py:66\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[1;32m---> 66\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql_query(text(sql_query), connection)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Log a message or handle the empty DataFrame scenario as needed\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    527\u001b[0m         sql,\n\u001b[0;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    535\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\pandas\\io\\sql.py:1839\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1784\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;124;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1837\u001b[0m \n\u001b[0;32m   1838\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1839\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   1840\u001b[0m     columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\pandas\\io\\sql.py:1663\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexec_driver_sql(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m-> 1663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1418\u001b[0m         distilled_parameters,\n\u001b[0;32m   1419\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1420\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py:517\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[1;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\u001b[38;5;241m.\u001b[39m_execute_clauseelement(\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m, distilled_params, execution_options\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1639\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[1;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[0;32m   1627\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[0;32m   1629\u001b[0m )\n\u001b[0;32m   1631\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[0;32m   1632\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[0;32m   1633\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1637\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[0;32m   1638\u001b[0m )\n\u001b[1;32m-> 1639\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_context(\n\u001b[0;32m   1640\u001b[0m     dialect,\n\u001b[0;32m   1641\u001b[0m     dialect\u001b[38;5;241m.\u001b[39mexecution_ctx_cls\u001b[38;5;241m.\u001b[39m_init_compiled,\n\u001b[0;32m   1642\u001b[0m     compiled_sql,\n\u001b[0;32m   1643\u001b[0m     distilled_parameters,\n\u001b[0;32m   1644\u001b[0m     execution_options,\n\u001b[0;32m   1645\u001b[0m     compiled_sql,\n\u001b[0;32m   1646\u001b[0m     distilled_parameters,\n\u001b[0;32m   1647\u001b[0m     elem,\n\u001b[0;32m   1648\u001b[0m     extracted_params,\n\u001b[0;32m   1649\u001b[0m     cache_hit\u001b[38;5;241m=\u001b[39mcache_hit,\n\u001b[0;32m   1650\u001b[0m )\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1654\u001b[0m         elem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m         ret,\n\u001b[0;32m   1659\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1848\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(\n\u001b[0;32m   1844\u001b[0m         dialect,\n\u001b[0;32m   1845\u001b[0m         context,\n\u001b[0;32m   1846\u001b[0m     )\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_single_context(\n\u001b[0;32m   1849\u001b[0m         dialect, context, statement, parameters\n\u001b[0;32m   1850\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1988\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1985\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(\n\u001b[0;32m   1989\u001b[0m         e, str_statement, effective_parameters, cursor, context\n\u001b[0;32m   1990\u001b[0m     )\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2344\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2343\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1969\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1967\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[0;32m   1970\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[0;32m   1971\u001b[0m         )\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1975\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1976\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1980\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1981\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:922\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 922\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[1;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: geographic_features\n[SQL: \n            SELECT *\n            FROM geographic_features\n            LEFT JOIN weather_features USING (Field_ID)\n            LEFT JOIN soil_and_crop_features USING (Field_ID)\n            LEFT JOIN farm_management_features USING (Field_ID)\n            ]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# Read the database, and clean the data using the processing modules we built.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "# from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
    "    \"regex_patterns\" : {\n",
    "            'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "            'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "            'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "            },\n",
    "}\n",
    "# Ignoring the field data for now.\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "# We're not going to use the weather data this time, so we'll ignore it.\n",
    "# weather_processor = WeatherDataProcessor(config_params)\n",
    "# weather_processor.process()\n",
    "# weather_df = weather_processor.weather_df\n",
    "\n",
    "dataset = field_df.drop(\"Weather_station\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at our dataset to ensure it imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into our analysis, it's crucial to ensure the integrity of our dataset and that the data are still as we expect it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the data\n",
    "# !pip install pytest\n",
    "\n",
    "dataset.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "import os# Define the file paths\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin our exploration of the dataset, we will use a decision tree to explore how all of the variables affect our crop yields.\n",
    "\n",
    "Given our dataset containing information on environmental factors and crop yields, how can we train a decision tree model to predict crop yields accurately?\n",
    "\n",
    "Fit a decision tree regressor model to the entire dataset.\n",
    "\n",
    "Use all predictor variables (excluding non-numeric columns) to predict `Standard_yield`. \n",
    "\n",
    "Set the `max_depth` to 4 and the `random_state` to 42 for reproducibility.\n",
    "\n",
    "Considering the **MSE** and **R²** values, how effective is the decision tree in predicting crop yields using all of our variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "While the model shows some predictive capability with a moderate R² value, there is still room for improvement, particularly in reducing the MSE and potentially increasing the explanatory power of the model. This can be achieved through further feature selection, hyperparameter tuning, or exploring alternative modelling techniques.\n",
    "\n",
    "Let's start with feature selection. What environmental factors have the highest correlation with crop yields, and how can we use this information to improve the predictive performance of our model?\n",
    "\n",
    "Conduct feature selection using a **correlation matrix** to select relevant features for predicting `Standard_yield`.\n",
    "\n",
    "Visualise the matrix to better identify the features with the highest correlation with `Standard_yield`.\n",
    "\n",
    "Regarding features to retain and eliminate, what do the results suggest? Have you thought about the independent features that might be correlated? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Now that we know which features could work best for our model, let's run our model again on a reduced dataset.\n",
    "\n",
    "Reduce the dataset to only include `Pollution_level`, `Min_temperature_C`, `Longitude`, and `pH`.\n",
    "\n",
    "Refit the decision tree regressor model using the reduced dataset and evaluate the model's performance on the test set to determine if there's an improvement in MSE and R² values.\n",
    "\n",
    "Does reducing the dataset lead to an improvement in the predictive performance of our model?\n",
    "\n",
    "Run the model on different combinations of the reduced dataset. Is there a combination of variables or a single variable that improves the MSE and R²?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Now that we have attempted feature engineering, let's move on to parameter tuning. What is the optimal depth for the decision tree model that maximises predictive performance while avoiding overfitting?\n",
    "\n",
    "Use a loop to fit decision tree models with depths ranging from 1 to 10 and assess the MSE and R² scores for each depth.\n",
    "Analyse the results to identify the depth that maximises R² while avoiding significant increases in MSE, indicating optimal model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop non-numeric columns\n",
    "non_numeric_columns = ['Location', 'Soil_type', 'Crop_type']\n",
    "#split data into dependent and independent variables\n",
    "X = dataset.drop(columns=['Standard_yield'] + non_numeric_columns)\n",
    "y = dataset['Standard_yield']\n",
    "    \n",
    "# Split the dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "# Train the model\n",
    "model = DecisionTreeRegressor(max_depth=4, random_state=42).fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "    \n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "result = {'mse': mse, 'r2': r2, 'model_depth': model.get_depth()}\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model shows some predictive capability with a moderate R² value, there is still room for improvement, particularly in reducing the MSE and potentially increasing the explanatory power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Correlation Analysis\n",
    "non_numeric_dataset = dataset.drop(columns = ['Location', 'Soil_type', 'Crop_type'])\n",
    "correlation_matrix = non_numeric_dataset.corr()\n",
    "print(\"Correlation with Standard_yield:\\n\", correlation_matrix['Standard_yield'].sort_values(ascending=False))\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the resulting correlation matrix, we would retain the variables with comparatively higher positive or negative absolute correlation coefficients, such as Annual_yield, Min_temperature_C, pH, Pollution_level, and Soil_fertility, and exclude the ones with lower correlation coefficients. It is however important to also use some knowledge we have in how the Standard_yield variable was created – we used Annual_yield as an input, and as such, including it as a predictive variable should be carefully thought about. Let's see what happens if we don't include it for now.  \n",
    "\n",
    "This will help us retain the most influential features for predicting crop yields while reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the features to keep based on correlation analysis\n",
    "features_to_keep = ['Pollution_level','Min_temperature_C','Longitude','pH']\n",
    "\n",
    "# Prepare the dataset with only the selected features\n",
    "X = dataset[features_to_keep]\n",
    "y = dataset['Standard_yield']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "model = DecisionTreeRegressor(max_depth=4, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Store the results\n",
    "result = {'mse': mse, 'r2': r2, 'model_depth': model.get_depth()}\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model's performance has not improved. The MSE has increased and the R-squared value has decreased. This indicates that using only the selected features has led to a worse predictive performance compared to the previous model. This illustrates the cost of simplified models – we're losing predictive ability whilst gaining ease of interpretation. It's also worth remembering that the R-squared value generally increases as more variables are added – which shouldn't tempt us to do so without good reason!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of depths to try\n",
    "depths_to_try = range(1, 11)\n",
    "\n",
    "# Initialise empty lists to store results\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "# Iterate over depths and fit models\n",
    "for depth in depths_to_try:\n",
    "    # Train the model\n",
    "    model = DecisionTreeRegressor(max_depth=depth, random_state=42).fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Append scores to lists\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Print the results\n",
    "for depth, mse, r2 in zip(depths_to_try, mse_scores, r2_scores):\n",
    "    print(f\"Depth: {depth}, MSE: {mse}, R2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the depth of the decision tree increases, the MSE generally decreases initially until reaching a minimum at depth 6. Beyond that, the MSE starts to increase again.\n",
    "\n",
    "The R-squared values also show a similar trend to MSE, initially increasing until depth 6 is reached, and then slightly decreasing afterwards.\n",
    "\n",
    "Depth 6 seems to provide the best balance between model complexity and performance, as it has the lowest MSE among depths 1 to 5 and a relatively high R² value compared to other depths.\n",
    "\n",
    "Beyond depth 6, increasing the depth of the decision tree leads to overfitting, as indicated by the increasing MSE and decreasing R-squared values. This means the model starts capturing noise in the data rather than true patterns, resulting in poorer performance on unseen data.\n",
    "\n",
    "As a final piece of testing (see it as a reward), select only the cassava crops and fit a decision tree to it. Tip: add in the Rainfall variable. The R-squared value is significantly higher than we had for our model. This suggests that we might need to break down the model into decision trees for each crop or use dummy variables when creating it. Sometimes it's useful to approach the same data from different angles, as a fitted model can probably always be improved! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
