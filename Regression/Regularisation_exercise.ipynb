{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0d358",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f089c81",
   "metadata": {},
   "source": [
    "# Exercise: Regularisation\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c026725",
   "metadata": {},
   "source": [
    "In this train, we'll focus on the practical application of various regularisation methods to prevent overfitting in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebbdb6",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "- Apply data scaling, ridge regression, and LASSO regression techniques to prevent overfitting and enhance model performance using real-world environmental data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97cee55",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64631b9e",
   "metadata": {},
   "source": [
    "> **Note**: Here is the link to the dataset to be used: https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc34d6",
   "metadata": {},
   "source": [
    "### Exercise 1: Data scaling\n",
    "Before applying regularisation techniques, it's crucial to scale the data. Therefore, scale the features of the `SDG_15_Life_on_Land_Dataset` to have a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Load the dataset and select features for scaling (exclude the `Year` column).\n",
    "- Implement standard scaling on the selected features.\n",
    "- Display the first five rows of the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9cf51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13014588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WaterQualityIndex  ClimateChangeImpactScore  LandUseChange  \\\n",
      "0          -0.509823                  0.915895       0.532798   \n",
      "1          -1.261473                 -1.159761       0.479063   \n",
      "2          -1.363971                 -1.409483       1.389846   \n",
      "3          -0.475658                  0.746916       0.684528   \n",
      "4          -0.885648                  1.230038      -0.213905   \n",
      "\n",
      "   InvasiveSpeciesCount  ConservationFunding  EcoTourismImpact  \\\n",
      "0              0.967295            -0.129430         -1.297085   \n",
      "1              1.382383            -1.098165          1.226669   \n",
      "2              0.206299             0.320340         -0.529103   \n",
      "3              0.828932             1.323673          1.653728   \n",
      "4              1.105658             1.323673          1.445945   \n",
      "\n",
      "   ForestCoverChange  SoilQualityIndex  WaterUsage  RenewableEnergyUsage  \\\n",
      "0           0.017923          0.689812   -0.641157             -1.290990   \n",
      "1          -1.649745          0.655167    0.539995              0.207271   \n",
      "2          -0.877370          0.759101    1.165311             -0.473757   \n",
      "3           1.188117          0.481943    1.165311              1.535274   \n",
      "4          -0.439815         -1.319584   -1.787570              1.160709   \n",
      "\n",
      "   CarbonEmissionLevels  AgriculturalIntensity  HabitatConnectivity  \\\n",
      "0             -0.930835              -1.237558            -1.131411   \n",
      "1              0.470716              -0.670150             0.305779   \n",
      "2             -0.110415               1.006319             1.598836   \n",
      "3              0.368164              -1.360736             0.001642   \n",
      "4              0.402348               1.675792            -0.977884   \n",
      "\n",
      "   SpeciesReintroductionEfforts  PollinatorDiversity  BiodiversityHealthIndex  \n",
      "0                      1.494660            -0.811078                -1.051696  \n",
      "1                     -0.107952             0.797582                 0.102687  \n",
      "2                     -1.017291            -1.518029                 0.656517  \n",
      "3                     -0.978326            -1.249998                 1.497105  \n",
      "4                      0.832482             1.077254                -1.288485  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv\")\n",
    "df.head()\n",
    "#select features to scale \n",
    "features_to_scale = df.drop('Year', axis=1)\n",
    "#scale \n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_to_scale)\n",
    "# Convert scaled features back to DataFrame for display\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features_to_scale.columns)\n",
    "\n",
    "# Display the first five rows\n",
    "print(scaled_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773c59b",
   "metadata": {},
   "source": [
    "### Exercise 2: Ridge regression\n",
    "Ridge regression is a technique used to analyse multiple regression data that suffer from multicollinearity. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "- Use the scaled features from **Exercise 1** as your predictors and select a suitable target variable from the dataset.\n",
    "- Split the data into training and test sets.\n",
    "- Implement a ridge regression model, with cross-validation to find the optimal regularisation parameter.\n",
    "- Evaluate the model on the test set and report the R-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "febadcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df.head()\n",
    "y =df['BiodiversityHealthIndex']\n",
    "X = scaled_features_df.drop('BiodiversityHealthIndex', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654769c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal regularisation parameter: {'alpha': 20}\n",
      "R-squared value on the test set: -0.012023387302823707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1,shuffle=False)\n",
    "\n",
    "# Ridge regression model with cross-validation\n",
    "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n",
    "ridge_regressor = GridSearchCV(Ridge(), parameters, scoring='r2', cv=5)\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = ridge_regressor.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Optimal regularisation parameter: {ridge_regressor.best_params_}\")\n",
    "print(f\"R-squared value on the test set: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3f4e4",
   "metadata": {},
   "source": [
    "### Exercise 3: LASSO regression\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean.\n",
    "\n",
    "Task:\n",
    "\n",
    "- Reuse the scaled features and target variable from the previous exercises.\n",
    "- Split the data into training and test sets.\n",
    "- Implement a LASSO regression model, with cross-validation to find the optimal regularisation parameter.\n",
    "- Evaluate the model on the test set and report the R-squared value and the number of features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4be3089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal regularisation parameter: 0.020234232153547617\n",
      "R-squared value on the test set: -0.0022728937499865154\n",
      "Number of features used: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "\n",
    "y =df['BiodiversityHealthIndex']\n",
    "X = scaled_features_df.drop('BiodiversityHealthIndex', axis = 1)\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42,shuffle=False)\n",
    "\n",
    "# LASSO regression model with cross-validation\n",
    "lasso_regressor = LassoCV(cv=5, random_state=42)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_lasso = lasso_regressor.predict(X_test)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "features_used = np.sum(lasso_regressor.coef_ != 0)\n",
    "\n",
    "print(f\"Optimal regularisation parameter: {lasso_regressor.alpha_}\")\n",
    "print(f\"R-squared value on the test set: {r2_lasso}\")\n",
    "print(f\"Number of features used: {features_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07792170",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2346b25",
   "metadata": {},
   "source": [
    "### Exercise 1: Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa44035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv')\n",
    "\n",
    "# Select features for scaling\n",
    "features = data.drop('Year', axis=1)\n",
    "\n",
    "# Implement standard scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert scaled features back to DataFrame for display\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Display the first five rows\n",
    "print(scaled_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d1cee",
   "metadata": {},
   "source": [
    "In this section, the dataset `SDG_15_Life_on_Land_Dataset` is loaded and preprocessed. The preprocessing involves scaling the features (excluding the `Year` column) to have a mean of 0 and a standard deviation of 1. (Try checking it!) This standard scaling is crucial for regularisation techniques, such as ridge and LASSO regression, as it ensures all features contribute equally to the model's performance. The exercise demonstrates how to use `StandardScaler` from `sklearn.preprocessing` to scale the dataset's features and displays the first five rows of the scaled features to verify the scaling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2382a39",
   "metadata": {},
   "source": [
    "### Exercise 2: Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Make 'BiodiversityHealthIndex' the target variable\n",
    "X = scaled_features_df\n",
    "y = data['BiodiversityHealthIndex']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ridge regression model with cross-validation\n",
    "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n",
    "ridge_regressor = GridSearchCV(Ridge(), parameters, scoring='r2', cv=5)\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = ridge_regressor.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Optimal regularisation parameter: {ridge_regressor.best_params_}\")\n",
    "print(f\"R-squared value on the test set: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24325ad",
   "metadata": {},
   "source": [
    "The ridge regression technique is particularly useful when dealing with multicollinearity, helping to reduce the variance of the coefficients and improve the model's generalisation ability. The task involves selecting a target variable, splitting the data into training and test sets, implementing ridge regression with cross-validation to find the best regularisation parameter (`alpha`), and finally, evaluating the model's performance on the test set using the R-squared metric. The R-squared value will indicate how well the model explains the variance in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f319c",
   "metadata": {},
   "source": [
    "**Interpretation**: The R-squared value reported from the ridge regression model evaluation signifies the proportion of the variance in the dependent variable that is predictable from the independent variables. In simple terms, it reflects the goodness of fit of the model. A value closer to 1 indicates a model that perfectly predicts the target variable, whereas a value closer to 0 indicates a model that fails to accurately predict the target variable's variance. It should be noted that an R-squared value of 1 suggests possible overfitting and it should be treated with caution. Investigating the individual variables and their coefficients might add more insight to whether the model has been overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc686b9",
   "metadata": {},
   "source": [
    "### Exercise 3: LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "\n",
    "# LASSO regression model with cross-validation\n",
    "lasso_regressor = LassoCV(cv=5, random_state=42)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_lasso = lasso_regressor.predict(X_test)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "features_used = np.sum(lasso_regressor.coef_ != 0)\n",
    "\n",
    "print(f\"Optimal regularisation parameter: {lasso_regressor.alpha_}\")\n",
    "print(f\"R-squared value on the test set: {r2_lasso}\")\n",
    "print(f\"Number of features used: {features_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba46c6",
   "metadata": {},
   "source": [
    "LASSO regression is beneficial for models that suffer from multicollinearity or when it's necessary to reduce the number of features in a model. LASSO does this by applying a penalty to the absolute size of the coefficients, effectively shrinking some of them to zero and thus excluding them from the model. This exercise involves reusing the scaled features and target variable, splitting the dataset, implementing LASSO regression with cross-validation to identify the optimal alpha, and evaluating the model. The output includes the R-squared value, which assesses the model's fit, and the count of features used, demonstrating LASSO's capability for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d25562",
   "metadata": {},
   "source": [
    "**Interpretation**: The R-squared value here, similar to ridge regression, measures how well the observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. Moreover, the number of features used by the LASSO model highlights its ability to perform feature selection, reducing the complexity of the model and potentially enhancing its interpretability by retaining only the most informative predictors. The reduction in the number of features used in this LASSO model suggests that we should be reassessing whether the features included in the ridge regression model were really adding value. Try to run the ridge regression again with fewer feature variables, including only the relevant features from the LASSO model to see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ccbde",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
