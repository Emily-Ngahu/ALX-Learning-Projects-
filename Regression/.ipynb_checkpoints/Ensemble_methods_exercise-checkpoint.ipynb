{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad6d9bc",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b10e19",
   "metadata": {},
   "source": [
    "# Exercise: Ensemble methods\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc90aa",
   "metadata": {},
   "source": [
    "In this train, we'll implement various ensemble methods using the Python scikit-learn library to enhance prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88dbc0",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "- Identify and apply different ensemble techniques within the scikit-learn framework.\n",
    "- Execute an ensemble method to improve the performance of a predictive model on a given dataset.\n",
    "- Evaluate and compare the effectiveness of ensemble methods versus single model approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ceb93",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d7f15",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploratory data analysis (EDA)\n",
    "\n",
    "Before we dive into ensemble methods, it's crucial to start with a robust feature selection process. This will help ensure that our models are only trained on variables that have a significant impact on our predictive variable, `BiodiversityHealthIndex`.\n",
    "\n",
    "Conduct a thorough EDA to understand the relationships between `BiodiversityHealthIndex` and other environmental indicators in the dataset. Look for patterns, anomalies, or any insights that can inform your feature selection process.\n",
    "\n",
    "Steps:\n",
    "1. Load and inspect the dataset (URL: `https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv`). \n",
    "2. Generate summary statistics for numerical features. \n",
    "3. Visualise the distribution of the `BiodiversityHealthIndex`.  \n",
    "4. Use a pairplot to visualise the relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b670c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "data = pd.csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca534dc",
   "metadata": {},
   "source": [
    "### Exercise 2: Correlation analysis\n",
    "Calculate the correlation coefficients between `BiodiversityHealthIndex` and each of the other environmental indicators. This will help us identify the variables most closely related to our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685250cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9de4c",
   "metadata": {},
   "source": [
    "### Exercise 3: Selecting top variables and adjusting the dataset\n",
    "Based on the correlation analysis, select the top 6 variables (`top_variables`) that show the highest correlation with `BiodiversityHealthIndex`. Adjust our dataset to include only the selected 6 variables and `BiodiversityHealthIndex`. This refined dataset will be used for training and testing our ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3807e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c7187",
   "metadata": {},
   "source": [
    "### Exercise 4: `RandomForestRegressor`\n",
    "\n",
    "Implement a `RandomForestRegressor` from `sklearn.ensemble` to predict `BiodiversityHealthIndex` based on other environmental indicators. Split the data into training and test sets, fit the `RandomForestRegressor` model on the training data, and calculate the R-squared value on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b399e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370d8c1",
   "metadata": {},
   "source": [
    "### Exercise 5: `GradientBoostingRegressor`\n",
    "\n",
    "Introduce `GradientBoostingRegressor` to predict `BiodiversityHealthIndex` using the same dataset. Train the model on the same training data and evaluate its R-squared value on the test set. Compare its performance with the `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9dfdb",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b4c16",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploratory data analysis (EDA)\n",
    "\n",
    "**Explanation**: This exercise introduces data analysis and visualisation techniques using `Pandas`, `Matplotlib`, and `Seaborn` libraries in Python. \n",
    "\n",
    "It starts by loading a biodiversity dataset with Pandas, providing a quick look at its structure through the initial rows and summary statistics. This foundation helps identify data characteristics and potential irregularities. \n",
    "\n",
    "The script then visualises the distribution of the `BiodiversityHealthIndex` using `Seaborn`, offering insights into its frequency across the dataset. \n",
    "\n",
    "Finally, it employs `Seaborn`'s `pairplot` on a dataset sample to explore relationships between variables, which is crucial for identifying correlations and trends that could inform further analysis. \n",
    "\n",
    "This approach outlines the importance of preliminary data exploration and visualisation in uncovering underlying patterns and guiding subsequent analytical steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d772c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Summary statistics for numerical features\n",
    "print(data.describe())\n",
    "\n",
    "# Visualising the distribution of the BiodiversityHealthIndex\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.displot(data['BiodiversityHealthIndex'])\n",
    "plt.title('Distribution of BiodiversityHealthIndex')\n",
    "plt.xlabel('BiodiversityHealthIndex')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualise the relationships between variables\n",
    "sns.pairplot(data.sample(100))  # Adjust sample size as necessary\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9afca3",
   "metadata": {},
   "source": [
    "The **summary statistics table** provides us with a general 'feel' of the data. There is nothing particularly concerning, looking at the values. Something to note is that the year column was picked up as numeric, and we'll mostly likely drop that going forward. \n",
    "\n",
    "**The distribution** of the BiodiversityHealthIndex shows that the values are distributed quite evenly between 0 and  1, with only two small spikes. \n",
    "\n",
    "**The pairplot** is where we get most of our information to build our model - but it looks as if might be a tough one, considering that there are no clear patterns or trends visible, not between the BiodiversityHealthIndex and the other variables, or even between any of the other variables. But let's see where it goes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296415e",
   "metadata": {},
   "source": [
    "### Exercise 2: Correlation analysis\n",
    "\n",
    "**Explanation**: This exercise calculates and visualises the correlation between all variables in a dataset, emphasising those related to `BiodiversityHealthIndex`. The correlation matrix is visualised using a heatmap, making it easy to identify which variables have the strongest linear relationships with `BiodiversityHealthIndex`. This process aids in feature selection by pinpointing the most influential variables for predictive modeling, thereby streamlining the development of more accurate and efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Variables')\n",
    "plt.show()\n",
    "\n",
    "# Focus on correlations with BiodiversityHealthIndex\n",
    "bhi_corr = corr_matrix['BiodiversityHealthIndex'].sort_values(ascending=False)\n",
    "print(bhi_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee543ec",
   "metadata": {},
   "source": [
    "The **correlation matrix** confirms what the pairplot suggested - there's very little correlation between the independent variables and the biodiversity index. Considering that strong correlation is at a value of 1, and our variables are much, much smaller at values closer to zero, there might be problems fitting a model. We'll forge ahead for now and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96cbde3",
   "metadata": {},
   "source": [
    "### Exercise 3: Selecting top variables and adjusting the dataset\n",
    "\n",
    "**Explanation**: This exercise narrows down the dataset to the six most impactful variables for predicting `BiodiversityHealthIndex`, based on previous correlation analysis. By focusing on the top 6 variables, it aims to enhance model accuracy and simplicity. The last command previews the refined dataset, ensuring the adjustments align with our predictive modelling objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names from our correlation analysis output above\n",
    "top_variables = ['SpeciesReintroductionEfforts', 'LandUseChange', 'ForestCoverChange', 'EcoTourismImpact', 'ConservationFunding', 'CarbonEmissionLevels']\n",
    "\n",
    "# Adjusting the dataset\n",
    "X = data[top_variables]\n",
    "y = data['BiodiversityHealthIndex']\n",
    "\n",
    "# Display the first few rows of the adjusted dataset\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ef99a",
   "metadata": {},
   "source": [
    "### Exercise 4: `RandomForestRegressor`\n",
    "\n",
    "**Explanation**: In this exercise, we train a `RandomForestRegressor`, a type of ensemble machine learning model, to predict outcomes based on input variables (`X`). It splits data into training and test sets to ensure model validation on unseen data, trains the model, and then evaluates its accuracy using the R-squared metric, which measures how well the model's predictions match the actual values. The use of `random_state` ensures reproducibility in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1793fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialise the RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"R-squared of the RandomForestRegressor: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd3cea",
   "metadata": {},
   "source": [
    "Well. This brings us to an uncomfortable reality of building models. We have a negative R-squared value! This is a big red flag, and should never be ignored (and the model should never be used as is). This suggests that there is not enough predictive powers in our model to predict with success. Considering that we know there is very little correlation between the input variables and the actual output variable that we could see earlier, this shouldn't come as a surprise. \n",
    "\n",
    "The obvious question now: What do we do? In short, some data are just not made for fitting models - we cannot change the fact that we have no predictors, even though we have a fantastic algorithm to use. Normally, we could play around to see what's happening and maybe find some relationships, but given the lack of patterns, linearity in relationships and general lack of correlation, it's unlikely to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01249e",
   "metadata": {},
   "source": [
    "### Exercise 5: `GradientBoostingRegressor`\n",
    "\n",
    "**Explanation**: The R-squared value indicates how well the model predictions approximate the real data points. Both RandomForestRegressor and GradientBoostingRegressor are powerful ensemble techniques for regression tasks. Gradient Boosting, by sequentially correcting errors of the previous trees, can potentially achieve higher accuracy. The comparison of their R-squared values will show which model is more effective in this context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b43137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialise the GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set with GradientBoostingRegressor\n",
    "y_pred_gb = gb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for GradientBoostingRegressor\n",
    "r_squared_gb = r2_score(y_test, y_pred_gb)\n",
    "print(f\"R-squared of the GradientBoostingRegressor: {r_squared_gb}\")\n",
    "\n",
    "# Compare with RandomForestRegressor R-squared\n",
    "print(f\"R-squared of the RandomForestRegressor: {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cb9d2",
   "metadata": {},
   "source": [
    "When fitting the Gradient Boosting model, we are again faced with a **negative R-squared** value. This confirms our experience with the tree model - it's unlikely that this data, in its current form and with the current lack of clear relationships, will be able to predict using either of these models. \n",
    "\n",
    "A large part of fitting models to data is knowing when the model is acceptable, and when it's almost worse than just using a moving average to suggest future paths. Today was unfortunately one of those days where we'll need to find a different solution, as neither of these models are fit for predicting outputs. It's important to be truthful about models fitting poorly, and about models not fitting at all (as we had today). Use the metrics produced to guide you, use the assumptions we introduced in earlier lessons to assess whether data might be appropriate for fitting a model, and only then start the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d0392",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
