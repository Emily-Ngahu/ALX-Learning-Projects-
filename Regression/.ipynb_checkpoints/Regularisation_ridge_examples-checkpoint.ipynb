{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d0c55a",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Regularisation – ridge\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this notebook, we'll take an in-depth look at regularisation and its implementation using the ridge regression method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "- Understand what regularisation is and how to implement it using the ridge method.\n",
    "- Understand the code required to implement a ridge regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598990ff",
   "metadata": {},
   "source": [
    "## Regularisation in 5 minutes\n",
    "\n",
    "Regularisation is like having a magic wand to make our predictions better in a smarter way. \n",
    "\n",
    "Imagine we're baking a cake and we have a bunch of ingredients. Each ingredient (like flour, eggs, sugar) affects how the cake turns out. Similarly, in making predictions, we have different factors, or \"ingredients\", that affect our predictions.\n",
    "\n",
    "Now, think of each ingredient as having its own importance level. For instance, in baking, maybe we need more flour than sugar. Similarly, in predicting, some factors might be more important than others.\n",
    "\n",
    "In the past, we used to pick and choose which ingredients to use in our recipes. But with regularisation, we keep all the ingredients but adjust their amounts based on how important they are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434a2f4",
   "metadata": {},
   "source": [
    "Let's take a look at our recipe, or equation, for making predictions:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2$$\n",
    "\n",
    "This might look like a complicated recipe, but it's just a way of saying, \"Let's see how well our predictions match up with reality.\"\n",
    "\n",
    "Now, here's the cool part. Regularisation adds a little twist to our recipe. It says, \"Hey, let's not let any ingredient (or factor) get too big.\" Why? Because when one ingredient becomes too dominant, it can mess up the taste of our cake (or the accuracy of our predictions).\n",
    "\n",
    "So, we add a new rule to our recipe:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^pb_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17003302",
   "metadata": {},
   "source": [
    "The new part, $\\alpha\\sum_{j=1}^pb_j^2$, is like a secret sauce that helps us control the quantities of our ingredients. The $\\alpha$ is like a knob we can turn to decide how much we want to control the sizes.\n",
    "\n",
    "Imagine $\\alpha$ as the volume control on your music player. If you turn it up, it makes the ingredients smaller. If you turn it down, it lets them be bigger.\n",
    "\n",
    "This secret sauce helps us balance two things: making sure our predictions fit the real data well, but also making sure none of our ingredients (or factors) become too overwhelming.\n",
    "\n",
    "![tradeoff](https://raw.githubusercontent.com/Explore-AI/Pictures/master/tradeoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b908e7",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To begin, let's import a few Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f3adc",
   "metadata": {},
   "source": [
    "Next we'll load our data as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ff01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad390acf",
   "metadata": {},
   "source": [
    "We can take a look at the dimensions of the DataFrame to get an idea of the number of rows, _n_, and the number of predictors, _p_, which is equal to one less than the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10997afb",
   "metadata": {},
   "source": [
    "Our dataset contains various environmental indicators related to SDG 15, such as deforestation rates, protected area coverage, biodiversity indices, and other relevant variables. Our objective is to model an environmental outcome for  the health of biodiversity using these indicators. \n",
    "\n",
    "The mathematical representation of our model can be described as follows:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "In this formulation, $Y$ represents the response variable, which in our case is `BiodiversityHealthIndex`. This response variable is influenced by _p_ predictor variables ($X_1, X_2, ..., X_p$), each representing different environmental indicators relevant to SDG 15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6575333",
   "metadata": {},
   "source": [
    "### Review of data scaling\n",
    "\n",
    "Data scaling is a key step when preparing our data for a model, especially when using techniques like regularisation. Regularisation helps improve our model by not letting it rely too much on any single feature, and it does this by keeping the model's coefficients (essentially, the importance it assigns to each feature) in check. But here's the catch: the size of these coefficients can be influenced by two main things:\n",
    "\n",
    "1. **How strongly a feature (let's call it `x`) is connected to the outcome (or `y`)**. This is the kind of relationship we're interested in capturing because it tells us something meaningful about our data.\n",
    "2. **The scale of the feature values**. For instance, imagine we're measuring distance. If we switch from metres to kilometres, the numbers change drastically, even though we're talking about the same distances. This change can unfairly affect the size of the coefficients.\n",
    "\n",
    "We want our model to focus on the real relationships in the data, not get thrown off by how we happen to measure things. That's where data scaling comes in. It adjusts the features so they're on a similar scale, making it easier for our regularisation technique to do its job properly.\n",
    "\n",
    "A popular method for scaling is called **Z-score standardisation**. It tweaks our data so that the features have similar ranges, and it's smart enough to handle outliers (data points that are very different from the rest) gracefully. Let's put this into action by using standard scaling in our model.\n",
    "\n",
    "To do this, we'll use something called `StandardScaler()` from a library in Python known as `sklearn.preprocessing`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into predictors and response\n",
    "X = df.drop('BiodiversityHealthIndex', axis=1)\n",
    "y = df['BiodiversityHealthIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07175784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaler method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce241b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled predictor values into a DataFrame\n",
    "X_standardised = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "X_standardised.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b0fe0",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "For our analysis, we'll start by dividing our dataset into a training set and a testing set. Unlike time-series data where the chronological order is crucial, our dataset consists of independent observations related to various environmental factors. Therefore, we can randomly sample rows for the training and testing sets without worrying about their order.\n",
    "\n",
    "After splitting our dataset, we'll proceed to fit and evaluate our model. Initially, we'll create a `Ridge()` object using the default parameter settings, which means we'll start with an $\\\\alpha$ value of 1. This alpha parameter controls the strength of the regularisation applied to our model, with higher values leading to more significant regularisation. In a later tutorial, we'll learn about choosing a better value for this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e401276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test splitting function from sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba845999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test, being sure to use the standardised predictors\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardised, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ridge regression module from sklearn\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ridge model with alpha = 1\n",
    "ridge = Ridge(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ce8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model intercept value\n",
    "b0 = float(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ff1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model coefficient value\n",
    "coeff = pd.DataFrame(ridge.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c274427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept:\", float(b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the coefficients\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bad04",
   "metadata": {},
   "source": [
    "### Interpretation of the intercept and coefficients\n",
    "\n",
    "Since we've standardised the features in our dataset, we're now able to directly compare the coefficients with one another. This is because all the variables are scaled to the same unit. The intercept in our model represents the expected health of biodiversity when all the features (like deforestation rate, pollution levels, conservation funding, etc.) are at their average values. The coefficients tell us how the Biodiversity Health Index is expected to change with a one-unit increase in each **scaled feature value**.\n",
    "\n",
    "Variables with smaller coefficients can be seen as having a lesser impact on biodiversity health; they've been more heavily penalised in the regularisation process, suggesting they're less crucial in predicting biodiversity health in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dba616",
   "metadata": {},
   "source": [
    "## Assess the predictive accuracy of the model\n",
    "\n",
    "To evaluate how well our model predicts biodiversity health, we can use tools from the `sklearn.metrics` library. To see the effectiveness of regularisation, we'll also compare our results to a basic linear model that doesn't use regularisation. This comparison will help us understand the benefits of applying regularisation to our environmental conservation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e666164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create model object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics module\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training accuracy\n",
    "train_lm = lm.predict(X_train)\n",
    "train_ridge = ridge.predict(X_train)\n",
    "\n",
    "print('Training MSE')\n",
    "print('Linear:', metrics.mean_squared_error(y_train, train_lm))\n",
    "print('Ridge :', metrics.mean_squared_error(y_train, train_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lm = lm.predict(X_test)\n",
    "test_ridge = ridge.predict(X_test)\n",
    "\n",
    "print('Testing MSE')\n",
    "print('Linear:', metrics.mean_squared_error(y_test, test_lm))\n",
    "print('Ridge :', metrics.mean_squared_error(y_test, test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62341cd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The increase in training MSE is not anything to be worried about since we want to avoid overfitting on the training set. However, the MSE values need to be contextualised to understand the model's performance better. The magnitude of the MSE can be influenced by the scale of the data and the choice of the predictive variable. Thus, it is crucial to consider these factors when assessing the model's accuracy.\n",
    "\n",
    "As a final step, let's plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train = ridge.predict(X_standardised.iloc[:len(y_train)])\n",
    "y_pred_test = ridge.predict(X_standardised.iloc[len(y_train):])\n",
    "\n",
    "plt.figure(figsize=(14, 7))  # Plot size\n",
    "\n",
    "# Plot training predictions and actual values\n",
    "plt.scatter(np.arange(len(y_train)), y_pred_train, label='Predicted Training', alpha=0.5)\n",
    "plt.plot(np.arange(len(y_train)), y_train, label='Actual Training', linestyle='--', linewidth=1)\n",
    "\n",
    "# Plot testing predictions and actual values\n",
    "plt.scatter(np.arange(len(y_test)) + len(y_train), y_pred_test, label='Predicted Testing', alpha=0.5, color='red')\n",
    "plt.plot(np.arange(len(y_test)) + len(y_train), y_test, label='Actual Testing', linestyle='--', linewidth=1, color='blue')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Model Predictions vs Actual Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Biodiversity Health Index')\n",
    "plt.grid(True)  # Add gridlines\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3d508",
   "metadata": {},
   "source": [
    "The plot indicates that the Ridge regression model predicts both the training and testing data with moderate success, showing some alignment with the actual Biodiversity Health Index values. There is a consistent scatter across the range, suggesting the model's effectiveness in handling diverse data without significant overfitting. However, it is essential to continue assessing the model's performance, especially in the context of the MSE values discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc9924",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we have seen or been introduced to:\n",
    "\n",
    "- The concept of regularisation in general and ridge regularisation in particular.\n",
    "- Implementing a linear model using ridge regularisation.\n",
    "- Comparing the accuracy of a non-regularised linear regression model with a regularised one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73610e0b",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Links to additional resources to help with the understanding of concepts presented in the train.\n",
    "\n",
    "- [Video on ridge regression by the authors of ISLR](https://www.youtube.com/watch?v=cSKzqb0EKS0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
