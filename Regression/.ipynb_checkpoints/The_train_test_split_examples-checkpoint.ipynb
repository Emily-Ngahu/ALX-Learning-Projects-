{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: The train-test split\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will examine the need for unseen data when assessing the performance of a model and how we can simulate it by splitting our existing dataset in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this lesson, we will:\n",
    "\n",
    "* Know why and how to split a dataset into training and testing subsets.\n",
    "* Know what is meant by a validation set and why it is important.\n",
    "* Carry out the train-test split using `sklearn` in Python.\n",
    "* Be able to review the performance of a model using the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When fitting a machine learning model to some data, we ultimately intend to use that model to **make predictions/forecasts on real-world data**. \n",
    "\n",
    "Real-world data refer to data that the model has never come across – the model hasn't had the opportunity to ingest this data in any of the training runs. In order to validate our model (and objectively assess how well it performs), we need to **test it on 'new' data**.\n",
    "\n",
    "However, gathering unseen data is not a simple task: Any new data would need to be cleaned, wrangled, and annotated just like the original data in our dataset. The next best thing, then, is to use some portion of the existing dataset to **simulate** some real-world, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Two-way split\n",
    "\n",
    "This technique involves splitting a dataset into **two sets**:\n",
    "\n",
    "- **The training set:** Used for training the model.\n",
    "- **The testing set:** Used for testing the model.\n",
    "   \n",
    "We fit a model using the **training data** and then assess its accuracy using the **test set**.\n",
    "\n",
    "We can use an **80-20 split**, i.e. use 80% of the data for training and keep 20% aside for testing. In other words, the training set will contain 80% of the rows, or data points, and the remaining 20% of rows will be in the test set. \n",
    "\n",
    "> These rows are **selected at random**, to ensure that the mix of data in the training set is as close as possible to the mix in the test set.\n",
    "\n",
    "Other train-test ratios, such as 70-30, can be applied depending on the requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Three-way split\n",
    "\n",
    "This technique involves splitting a dataset into **three distinct parts**:\n",
    "- **Training set**\n",
    "- **Validation set**\n",
    "- **Testing set** \n",
    "\n",
    "The idea here is that, as before, the **training set** is used to fit the model to the observations. \n",
    "\n",
    "Thereafter, during the model tuning process where hyperparameters are tweaked and decisions on the dataset are made, the **validation set** is used to test the performance of the model and improve it.\n",
    "\n",
    "Once the model designer is satisfied with the performance of the model on the validation set, the previously unseen **test set** is brought out and used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "#### Caveats for using a validation set\n",
    "\n",
    "On small datasets, it may not be feasible to include a validation set for the following reasons:\n",
    "\n",
    "- The model may need every possible data point to adequately determine model values.\n",
    "- For small enough test sets, the uncertainty of the test set can be considerably large to the point where different test sets may produce very different results.\n",
    "\n",
    "Clearly, further splitting the training data into training and validation sets would remove precious observations for the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cross-validation\n",
    "\n",
    "In the case that the designer does not desire to use a designated validation set, or there is simply not enough data, a technique known as cross-validation may be used. \n",
    "\n",
    "A common version of cross-validation is known as **K-fold cross-validation** where the dataset is **randomly partitioned** into \n",
    "**k equally sized subsets** or \"folds\".\n",
    "\n",
    "During the training process, one of the folds is held back to be used as a validation set while the rest are used for training. \n",
    "\n",
    "The process is repeated k times with a different fold being held back each time. This is such that each fold serves as the validation set once, and as part of the training set k−1 times. \n",
    "\n",
    "The k results can then be averaged to produce a single estimation of model performance.\n",
    "\n",
    "Once the model has been validated using cross-validation, we then finally test it on the test set that was unseen by the model from the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The train-test split in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data on the **Rand exchange rates over time**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/regression_sprint_data_1.csv', index_col=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `train_test_split`  function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the split function from sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into the response, y, and features, X\n",
    "y = df['ZAR/USD']\n",
    "X = df.drop('ZAR/USD', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'train_test_split' function:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have handed **four parameters** to the splitting function:\n",
    "\n",
    "- **`X`:** Contains the features on which we will be training the model. In this case: `exports`.\n",
    "- **`y`:** This is the response variable which we are trying to predict. In this case: `exchange rate`.\n",
    "- **`test_size`:** This is a value between 0 and 1: the proportion of our dataset that we want to be used as test data. Typically `0.2` (20%).\n",
    "- **`random_state`:** This is an arbitrary value which, when set, ensures that the _random_ nature in which rows are picked to be in the test set is the same each time the split is carried out. In other words, the rows are picked at random, but we can ensure these random picks are repeatable by using the same value here. This makes it easier to assess model performance across iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and testing sets\n",
    "\n",
    "By plotting the data points in each of the training and testing sets in different colours, we should be able to see that we have a **similar _spread_ of data** in each (but far fewer data _points_ in the testing set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the splitting results\n",
    "plt.scatter(X_train, y_train, color='green', label='Training')  # plot the training data in green\n",
    "plt.scatter(X_test, y_test, color='darkblue', label='Testing')  # plot the testing data in blue\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear model\n",
    "\n",
    "Recall, model training is the process of exposing a model to our data such that it can learn the patterns and combinations of variables which result in a particular response value. For example, financial factors combining to produce an exchange rate.\n",
    "\n",
    "We will fit our model using the same steps as before, except this time we will **expose it only to the training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression module\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model object\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, as we fit the model, we provide it with the **training features and their responses**. This way, it can learn which data points map to which output, as is required of the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data \n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept, or y-cut, of our linear model\n",
    "a = float(lm.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient, or gradient, of our linear model\n",
    "b = lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope:\\t\\t\", b)\n",
    "print(\"Intercept:\\t\", float(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the model on the training data\n",
    "\n",
    "We can get an idea of what the regression line looks like by overlaying it on a scatter plot of the training data.\n",
    "\n",
    "In order to draw the line, we need to generate the set of points that belong to it. We can do this by using the `predict` method on the model object and having it predict `y` values from the `X` values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the values that fall along our regression line\n",
    "gen_y = lm.predict(X_train)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_train, y_train, color='green', label='Training data')  # Plot the training data in green\n",
    "plt.plot(X_train, gen_y, color='red', label='Regression line')  # Plot the line connecting the generated y-values\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assess the fit of the line to the training data using `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "# Calculate the mean-squared-error\n",
    "print('MSE:', metrics.mean_squared_error(y_train, gen_y))\n",
    "# Calculate the R-squared metric\n",
    "print('R_squared:', metrics.r2_score(y_train, gen_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the model on the testing (unseen) data\n",
    "\n",
    "Our model has been trained. Now we need to show it some data it's never seen before, and ask it to **generate some predictions from that unseen data**. \n",
    "\n",
    "In this instance, we don't provide the model with the `y` values (response variable), because that is akin to giving it the answers we are asking it to predict.\n",
    "\n",
    "Once it has made the predictions, we compare those predictions to the set of `y` values corresponding to the test set, but which the model has not seen. \n",
    "\n",
    "> Remember, a linear regression model is simply a straight line, and all predictions it makes will lie on that line.\n",
    "\n",
    "To make those predictions, we again use the `predict` method on the model object, but this time give it the **_test_ set** from which to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values of y from x, using the linear model\n",
    "gen_y_test = lm.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test, y_test, color='darkblue', label='Testing data')  # Plot the testing data in blue\n",
    "plt.plot(X_test, gen_y_test, color='red', label='Regression line')  # Plot the line connecting the generated y-values in red\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assess the fit of the line on the unseen testing data by checking **MSE and $R^2$ metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing:\")\n",
    "print('MSE:', metrics.mean_squared_error(y_test, gen_y_test))\n",
    "print('R_squared:', metrics.r2_score(y_test, gen_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "**Mean squared error** is higher on the test set than the train set, indicating poor predictive accuracy.\n",
    "\n",
    "**R-squared** is lower on the test set, indicating a worse fit on the test set.\n",
    "\n",
    "These results indicate a concept in machine learning model fitting known as _overfitting_. This is a phenomenon where there is:\n",
    "\n",
    "- A discrepancy between the performance of the model on train and on test sets.\n",
    "- An inability of the model to _generalise_ to data it has not seen before.\n",
    "\n",
    "The term comes from the fact that the model **fits too well, or overfits, the training data**, and does not fit well, or **underfits, the testing data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we have seen or been introduced to:\n",
    "- The concept of _unseen_ data, from the perspective of the model.\n",
    "- Splitting a dataset into training and testing subsets.\n",
    "- Calculating or interpreting model parameters (slope, intercept) using the training set.   \n",
    "- Assessing the accuracy and fit of the model on the testing set.\n",
    "\n",
    "Subsequently, we will look at methods for improving the predictive accuracy of our model, as well as mitigating overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
