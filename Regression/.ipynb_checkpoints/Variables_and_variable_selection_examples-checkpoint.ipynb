{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Variables and variable selection\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn about dummy variable encoding for categorical data and selecting features based on correlation and variance thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this lesson, we will:\n",
    "\n",
    "* Understand encoding dummy variables.\n",
    "* Know how to select variables using correlation and significance.\n",
    "* Know how to select variables using variable thresholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "  - [Introduction](#Introduction)\n",
    "  - [1. Variable types and summary statistics](#1.-Variable-types-and-summary-statistics)\n",
    "  - [2. Dummy variable encoding](#2.-Dummy-variable-encoding)\n",
    "  - [3. Correlations and model structure](#3.-Correlations-and-model-structure)\n",
    "  - [4. Variable selection by correlation and significance](#4.-Variable-selection-by-correlation-and-significance)\n",
    "  - [5. Variable selection by variance thresholds](#5.-Variable-selection-by-variance-thresholds)\n",
    "  - [6. Train and compare models on the reduced datasets](#6.-Train-and-compare-models-on-the-reduced-datasets)\n",
    "  - [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Variables** are the basic building blocks of datasets. The **quality of the variables** present within your dataset has a direct impact on the intuition and overall outcome of your machine learning model. \n",
    "\n",
    "Although similar machine models may be implemented within the same industry, this does not mean they necessarily require the same features. **Variable selection** and an in-depth knowledge of the domain you're building your model in remains essential when developing a predictive model.\n",
    "\n",
    "The purpose of regression is essentially to build associations between multiple variables. \n",
    "\n",
    "> Variable selection involves the **elimination of some input variables** which may in turn reduce the computational cost of modeling and, in some cases, improve the performance of the model. \n",
    "\n",
    "Within this train, we will investigate appropriate ways of performing this model selection. \n",
    "\n",
    "The model is structured around the belief that one of the variables in our dataset is a **dependent variable (DV)**, that is explained or predicted in some way by the other **independent variables (IVs)**. In this sense we work with: \n",
    "\n",
    "- **Input variables** - Referred to as the independent variables (IVs) and are used to explain or predict the target variable.\n",
    "\n",
    "- **Target variable** - Referred to as the dependent variable (DV) and is the target variable we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **import** a few Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `personal_loans` dataset which contains data of **bank customers**. The data includes basic information of each customer as well as whether the customer took out a loan and the size of that loan.\n",
    "\n",
    "The basic information consists of:\n",
    "* **Age** - Customer's age in years \n",
    "* **Experience** - working experience in years\n",
    "* **Income** - annual income expressed in multiples of 1000\n",
    "* **Family** - members in family including the customer self\n",
    "* **CCAvg** - average monthly spend on credit card\n",
    "* **Education** - Undergrad/Postgrad/Professional\n",
    "* **Mortgage** - amount expressed in multiples of 1000'\n",
    "* **Securities Account** - whether the customer has a securities account\n",
    "* **CD Account** - whether the customer has a cash deposit account\n",
    "* **Online** - whether the customer is using online banking\n",
    "* **Gender** - Male/Female\n",
    "* **Area** - Geographic area where the customer lives \n",
    "* **Personal Loan** - whether the person took out a personal loan\n",
    "* **Loan Size** - amount expressed in multiples of  1000\n",
    "\n",
    "We will **load our data** as a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/bootcamps/Personal_Loans.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "With our data loaded, we now consider doing some preliminary data preprocessing. \n",
    "\n",
    "Some of the **columns have white space** that we want to replace with an underscore (to avoid using the column names as variable names later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.replace(\" \",\"_\") for col in df.columns] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to build some relationship between variables that are likely to indicate the loan amount once someone has taken a loan, we really only want to **consider customers who actually took a personal loan** to build this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Personal_Loan'] == 1]\n",
    "df = df.drop(['Personal_Loan'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variable types and summary statistics\n",
    "\n",
    "In this section, we will explore the data types and the summary statistics of our variables.\n",
    "\n",
    "Variables have **different levels** of inherent statistical and model building information. They may generally be grouped into the following types as seen in the image* below:\n",
    "\n",
    "> You can copy the image URL below into a new tab to get the full size version to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url='https://raw.githubusercontent.com/Explore-AI/Pictures/master/Variable_data_types.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the **data types and number of entries** of each column in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `df.info()` specifically outputs the number of non-null entries in each column. As such, we can be certain that our data has missing values if columns have a varying number of non-null entries.  \n",
    "\n",
    "Now let's look at a table showcasing the **summary statistics** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the means and standard deviations of different columns, we may want to consider [standardizing](https://statisticsbyjim.com/regression/standardize-variables-regression/) our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dummy variable encoding\n",
    "\n",
    "As observed in the above table, we get little information from the summary statistics of our numerical categorical data (`Online`, `CD_Account`, `Securities_Account`).\n",
    "\n",
    "More importantly, **all input data for regression model** building purposes **needs to be numerical**. \n",
    "\n",
    "We therefore have to **transform the text data**(found within columns such as `Education`, `Gender`, and `Area`) **into numbers** before we can train our machine learning model. \n",
    "\n",
    "To facilitate this transformation from textual-categorical data to numerical equivalents, we **use a pandas method** called `get_dummies`. \n",
    "\n",
    "`get_dummies` will transform all the categorical text data into numbers by adding a **column for each distinct category**. The new column has:\n",
    "- `1` for observations that **were** in this category.\n",
    "- `0` for observations that **were** not in this category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the dataframe:\n",
    "\n",
    "| Dog Age | Breed      |\n",
    "|---------|------------|\n",
    "| 15      | \"Bulldog\"  |\n",
    "| 12      | \"Labrador\" |\n",
    "| 10      | \"Labrador\" |\n",
    "| 22      | \"Beagle\"   |\n",
    "| 9       | \"Labrador\" |\n",
    "\n",
    "\n",
    "After `pd.dummies` becomes:\n",
    "\n",
    "| Dog Age | Breed_Labrador | Breed_Bulldog | Breed_Beagle |\n",
    "|---------|----------------|---------------|--------------|\n",
    "| 15      | 1              | 0             | 0            |\n",
    "| 12      | 0              | 1             | 0            |\n",
    "| 10      | 1              | 0             | 0            |\n",
    "| 22      | 0              | 0             | 1            |\n",
    "| 9       | 1              | 0             | 0            |\n",
    "\n",
    "\n",
    "In general, this is a process known as [Dummy Variable Encoding](https://en.wikiversity.org/wiki/Dummy_variable_(statistics)) and is an important step in preprocessing data for regression analysis.   \n",
    "\n",
    "\n",
    "Let's see what this process looks like with our personal loans dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy variable encoding our dataset\n",
    "\n",
    "df_dummies = pd.get_dummies(df)\n",
    "\n",
    "# Again we make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \",\"_\") for col in df_dummies.columns] \n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddenly we have many more variable columns - our **original 13 variable columns** are **now 44** given the dummy variable encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlations and model structure\n",
    "\n",
    "Using the dummy variable dataframe, we can build a model that predicts `Loan_Size` (our dependent variable) as a function of **43 different independent variables**.\n",
    "\n",
    "Before we do this, however, let's **reorder columns** so that our dependent variable is the last column of the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = [col for col in df_dummies.columns if col!= 'Loan_Size'] + ['Loan_Size']\n",
    "df_dummies=df_dummies.reindex(columns=column_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and visualise the correlation matrix\n",
    "\n",
    "This makes a **heatmap visualisation** representing a **correlation matrix** of our data easier to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation matrix\n",
    "df_dummies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation heatmap\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "fig = plt.figure(figsize=(15,15));\n",
    "ax = fig.add_subplot(111);\n",
    "plot_corr(df_dummies.corr(), xnames = df_dummies.corr().columns, ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the correlations that it's not the best idea to keep all of the dummy variables.\n",
    "\n",
    "If we use all of these variables, we're effectively working with **superfluous or redundant information**. \n",
    "\n",
    "Our model will also have **collinearity issues**:\n",
    "\n",
    "- `Gender_Male` and `Gender_Female` are perfectly negatively correlated\n",
    "\n",
    "This will likely be a problem when we build a model - let's check what an OLS model summary says."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model using `statsmodels.OLS`\n",
    "\n",
    "#### Generating the regression string\n",
    "\n",
    "We will be importing the `statsmodels` library which has a rich set of statistical tools to help us. \n",
    "\n",
    "Those of you familiar with the R language will know that fitting a machine learning model requires a sort of string of the form:\n",
    "\n",
    "`y ~ X`\n",
    "\n",
    "which is reads: **\"Regress y on X\"**. \n",
    "\n",
    "`statsmodels` works in a similar way, so we need to generate an appropriate string to feed to the method when we wish to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Model DataFrame with all of the columns:\n",
    "dfm = df_dummies.copy()\n",
    "\n",
    "# The dependent variable:\n",
    "y_name = 'Loan_Size'\n",
    "\n",
    "# The independent variable\n",
    "# (let's first try all of the columns in the model DataFrame)\n",
    "X_names = [col for col in dfm.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+\" ~ \"+\" + \".join(X_names);\n",
    "print('Formula:\\n\\t {}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a warning about strong multicollinearity. This is likely as a result of the incorrect filtering of one hot encoded dummy variables (we noticed earlier that `Gender_Male` and `Gender_Female` are perfectly negatively correlated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat dummy variable encoding with `drop_first` parameter\n",
    "\n",
    "In order to ensure that we don't assume an underlying relationship between the categories, we can call `pd.get_dummies` with the argument `drop_first=True` so that we only create **n-1** columns for each variable with **n** categories (i.e. one variable/column with five categories will be transformed into four columns of 0's and 1's). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Again make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \", \"_\") for col in df_dummies.columns]\n",
    "\n",
    "# Reorder columns with the dependent variable (claim_amount) the last column\n",
    "column_titles = [col for col in df_dummies.columns if col !=\n",
    "                 'Loan_Size'] + ['Loan_Size']\n",
    "df_dummies = df_dummies.reindex(columns=column_titles)\n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have **41** columns instead of **44**. This gives us 40 potential independent variables that could be used to build a relationship on `Loan_Size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS fit summary\n",
    "\n",
    "Let's check what the OLS model summary would say if we now **fit only the 41 variable columns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep the model DataFrame, but only specify the columns we want to fit this time\n",
    "X_names = [col for col in df_dummies.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+' ~ '+'+'.join(X_names)\n",
    "\n",
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the **condition number** has improved, but there is still mention of strong multicollinearity in warning **\\[2\\]**\n",
    "\n",
    "We also see that the Q1 - Q3 range of coefficients and expected errors are larger than the absolute size of the coefficients themselves.\n",
    "\n",
    "Let's make further selections on the variables now using their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variable selection by correlation and significance\n",
    "\n",
    "We now have 40 predictor variables to choose from, so we need a way of guiding us to **choose the best ones** to be our predictors. \n",
    "\n",
    "One way is to look at the **correlations** between the **`Loan Size` and each variables** in our DataFrame and select those with the **strongest correlations** - both positive and negative.\n",
    "\n",
    "We also need to consider **how significant** those features are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlation coefficents and p-values\n",
    "\n",
    "The code below will create a new DataFrame and store the correlation coefficents and p-values in that DataFrame for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between predictor variables and the response variable\n",
    "corrs = df_dummies.corr()['Loan_Size'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Pearson regression](http://sites.utexas.edu/sos/guided/inferential/numeric/bivariate/cor/) from SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Build a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'Loan_Size']\n",
    "\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_dummies[col], df_dummies['Loan_Size'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "    \n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got a **sorted list of the p-values and correlation coefficients** for each of the features, when considered on their own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the statistically significant features\n",
    "\n",
    "If we were to use a logic test with a significance value of 5% (**p-value < 0.05**), we could infer that the following features are statistically significant:\n",
    "\n",
    "* Income\n",
    "* Mortgage\n",
    "* CCAvg\n",
    "* Experience\n",
    "* Age\n",
    "* Education_Undergrad\n",
    "* Family\n",
    "\n",
    "Let's keep only the variables that have a significant correlation with the dependent variable. We'll put them into an independent variable DataFrame `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dependent variable remains the same:\n",
    "y_data = df_dummies[y_name]  # y_name = 'Loan_Size'\n",
    "\n",
    "# Model building - Independent Variable (IV) DataFrame\n",
    "X_names = list(df_cp[df_cp['P_Value'] < 0.05].index)\n",
    "X_data = df_dummies[X_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and removing the highly correlated features\n",
    "\n",
    "We also need to look for predictor variable pairs which have a **high correlation with each other** to avoid **autocorrelation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix\n",
    "corr = X_data.corr()\n",
    "\n",
    "# Find rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr[np.abs(corr) > 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the whole correlation matrix, it might be easier to isolate the sections of the correlation matrix to where the off-diagonal correlations are high - **greater than 0.9 or less than 0.9**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we create the correlation matrix\n",
    "# and find rows and columns where correlation coefficients > 0.9 or <-0.9\n",
    "corr = X_data.corr()\n",
    "r, c = np.where(np.abs(corr) > 0.9)\n",
    "\n",
    "# We are only interested in the off diagonal entries:\n",
    "off_diagonal = np.where(r != c)\n",
    "\n",
    "# Show the correlation matrix rows and columns where we have highly correlated off diagonal entries:\n",
    "corr.iloc[r[off_diagonal], c[off_diagonal]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it looks like `Age` and `Experience` are highly correlated (perhaps unsurprising if you take a moment to think about it).\n",
    "\n",
    "This is also visible looking back at the correlation coefficient heatmap and matrix from earlier, but a more focused / subset view of the matrix is useful to isolate the coefficients of interest.\n",
    "\n",
    "Considering which predictor variable to drop, `Experience` is slightly **better correlated** (and lower p-value) **to the dependent variable** `Loan Size`, so **let's drop** `Age` from the feature dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS fit summary\n",
    "\n",
    "Now let's see what the resulting OLS fit summary says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a new subset of our potential independent variables\n",
    "X_remove = ['Age']\n",
    "X_corr_names = [col for col in X_names if col not in X_remove]\n",
    "\n",
    "# Create our new OLS formula based-upon our smaller subset\n",
    "formula_str = y_name+' ~ '+' + '.join(X_corr_names);\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the OLS model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Display the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variable selection by variance thresholds\n",
    "\n",
    "Variance Thresholds remove **features whose values don't change much** from observation to observation. \n",
    "\n",
    "The objective here is to **remove** all features that have a **variance lower than** the selected **threshold**.\n",
    "\n",
    "For example, suppose that in our loans dataset 97% of observations were for 40-year-old women, then the `Age` and `Gender` features can be removed without a great loss in information.\n",
    "\n",
    "**Note:** Variance is dependent on scale, so the features will have to be **normalized** before implementing variance thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into independent (X) and independent (y) variables\n",
    "X_names = list(df_dummies.columns)\n",
    "X_names.remove(y_name)\n",
    "X_data = df_dummies[X_names]\n",
    "y_data = df_dummies[y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X_normalize = pd.DataFrame(X_scaled, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold in Scikit Learn\n",
    "\n",
    "To implement Variance Threshold in Scikit Learn we have to do the following:\n",
    "\n",
    "- Import and create an instance of the `VarianceThreshold` class\n",
    "- Use the `.fit()` method to select subset of features based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create VarianceThreshold object\n",
    "selector = VarianceThreshold(threshold=0.03)\n",
    "\n",
    "# Use the object to apply the threshold on data\n",
    "selector.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variance Threshold has been applied to the data. Let's look at the **calculated variance for each predictive variable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column variances\n",
    "column_variances = selector.variances_\n",
    "\n",
    "vars_dict = {}\n",
    "vars_dict = [{\"Variable_Name\": c_name, \"Variance\": c_var}\n",
    "             for c_name, c_var in zip(X_normalize.columns, column_variances)]\n",
    "df_vars = pd.DataFrame(vars_dict)\n",
    "df_vars.sort_values(by='Variance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the variances of the individual columns before any threshold is applied. It allows us to **revise our initial variance threshold** if we feel that we might exclude important variables.\n",
    "\n",
    "\n",
    "Next we need to **extract the results** and use them to **select our new columns** - which form a subset of all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select new columns\n",
    "X_new = X_normalize[X_normalize.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Save variable names for later\n",
    "X_var_names = X_new.columns\n",
    "\n",
    "# View first few entries\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a threshold of `0.03` we have have gone from **40 to 19** predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with different thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try **a few more thresholds** to see what we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variance Threshold objects\n",
    "selector_1 = VarianceThreshold(threshold=0.05)\n",
    "selector_2 = VarianceThreshold(threshold=0.1)\n",
    "selector_3 = VarianceThreshold(threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_1.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_2.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_3.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of columns\n",
    "X_1 = X_normalize[X_normalize.columns[selector_1.get_support(indices=True)]]\n",
    "X_2 = X_normalize[X_normalize.columns[selector_2.get_support(indices=True)]]\n",
    "X_3 = X_normalize[X_normalize.columns[selector_3.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the number of predictors for each threshold\n",
    "\n",
    "Now let's graph the number of predictors by the thresholds to investigate the relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(8, 3), nrows=1, ncols=1)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "subset_preds = [X_1.shape[1], X_2.shape[1], X_3.shape[1]]\n",
    "thresholds = ['0.05', '0.1', '0.15']\n",
    "\n",
    "# Plot graph\n",
    "ax.set_title('# of Predictors vs Thresholds')\n",
    "ax.set_ylabel('# of Predictors')\n",
    "ax.set_xlabel('Threshold')\n",
    "sns.barplot(x=thresholds, y=subset_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that as we **increase the threshold**, the **number of dimensions decrease**.\n",
    "\n",
    "> Can you extract the predictor names of the 3 different datasets above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS fit summary\n",
    "\n",
    "Let's see what the resulting OLS fit summary for a threshold of 0.03 says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our new OLS formula?\n",
    "formula_str = y_name+' ~ '+' + '.join(X_new.columns)\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=df_dummies)\n",
    "fitted = model.fit()\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages & Disadvantages of Variance Thresholds\n",
    "\n",
    "Let's consider some trade-offs associated with using variance thresholds for variable selection: \n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Applying variance thresholds is based on solid intuition: features that don't change much also don't add much information;\n",
    "* Easy and relatively safe way to reduce dimensionality (i.e. number of features) at the start of the modeling process.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Not the ideal algorithm if dimensionality reduction is not really required;\n",
    "* The threshold must be manually tuned, which can be a fickle process requiring domain/problem expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and compare models on the reduced datasets\n",
    "\n",
    "Now that we have thinned out our DataFrame using various methods, let's see if we can fit linear regression models and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the datasets\n",
    "\n",
    "First we need to make sure that all models are trained and tested on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                    y_data,\n",
    "                                                    test_size=0.20,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for variance threshold model\n",
    "X_var_train = X_train[X_var_names]\n",
    "X_var_test = X_test[X_var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for correlation threshold model\n",
    "X_corr_train = X_train[X_corr_names]\n",
    "X_corr_test = X_test[X_corr_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the models\n",
    "\n",
    "Next we instantiate and fit our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "lm = LinearRegression()\n",
    "lm_corr = LinearRegression()\n",
    "lm_var = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "lm.fit(X_train, y_train);\n",
    "lm_corr.fit(X_corr_train,y_train);\n",
    "lm_var.fit(X_var_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the intercepts and coeffiecients of the three models above, what do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the accuracy of the models\n",
    "\n",
    "Let's see how our linear models performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(15, 5), nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "train_pred = [lm.predict(X_train),\n",
    "              lm_corr.predict(X_corr_train),\n",
    "              lm_var.predict(X_var_train)]\n",
    "test_pred = [lm.predict(X_test),\n",
    "             lm_corr.predict(X_corr_test),\n",
    "             lm_var.predict(X_var_test)]\n",
    "title = ['No threshold', 'Corr threshold', 'Var threshold']\n",
    "\n",
    "# Key:\n",
    "# No threshold - linear regression with all predictive variables\n",
    "# Corr threshold - linear regression with correlation thresholded predictive variables\n",
    "# Var threshold - linear regression with variance thresholded predictive variables\n",
    "\n",
    "\n",
    "# Loop through all axes to plot each model's results\n",
    "for i in range(3):\n",
    "    test_mse = round(mean_squared_error(test_pred[i], y_test), 4)\n",
    "    test_r2 = round(r2_score(test_pred[i], y_test), 4)\n",
    "    train_mse = round(mean_squared_error(train_pred[i], y_train), 4)\n",
    "    train_r2 = round(r2_score(train_pred[i], y_train), 4)\n",
    "    title_str = f\"Linear Regression({title[i]}) \\n train MSE = {train_mse} \\n \" + \\\n",
    "                f\"test MSE = {test_mse} \\n training $R^{2}$ = {train_r2} \\n \" + \\\n",
    "                f\"test $R^{2}$ = {test_r2}\"\n",
    "    ax[i].set_title(title_str)\n",
    "    ax[i].set_xlabel('Actual')\n",
    "    ax[i].set_ylabel('Predicted')\n",
    "    ax[i].plot(y_test, y_test, 'r')\n",
    "    ax[i].scatter(y_test, test_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the results that we have managed to **slightly improve our model by using fewer predictors**. Sometimes, it would seem less really *is* more.   \n",
    "\n",
    "It's interesting to note that although our **training MSE** for the **\"No threshold\" model** was the **lowest at training**, it increases to the **highest** of the three models **at testing**. \n",
    "\n",
    "This is a sign that the model was overfitting the data, and the **other two models** have a **better capacity for generalising** to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we learned about variables and the some of the **different variable types** that can exist in the datasets we attempt to model. \n",
    "\n",
    "We also learned how to deal with categorical data types using **dummy variable encoding**. \n",
    "\n",
    "We were introduced to various **methods of variable selection and elimination**; particularly implementing the use of summary statistics, correlation, significance and variance threshold methods for variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
