{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Heterogeneous ensemble methods\n",
    " \n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we'll dive into heterogeneous ensemble methods, combining various model types to enhance prediction strength with a focus on implementing voting and stacking techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "* Combine diverse models for predictions.\n",
    "* Implement voting and stacking strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "Ensemble learning in machine learning is the practice of **combining multiple models** to try and achieve higher overall model performance. \n",
    "\n",
    "Ensembles can consist of multiple models trained on the same dataset. Each of these models is used to make predictions on the same input, then these predictions are aggregated across all models in some way (for example, by taking the mean) to produce the final output.   \n",
    "\n",
    "Other than the possibility of improving model performance, there are [lots of other reasons](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2) why we might want to build an ensemble. \n",
    "\n",
    "In this train, we explore some **heterogeneous ensemble** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogeneous ensembles\n",
    "\n",
    "This type of ensemble consists of **different types of models**, *so we can add pretty much any regression model we want*.  \n",
    "\n",
    "Here we investigate two ways of combining models which are different in nature: **voting** and **stacking**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the individual models\n",
    "\n",
    "Let's start by training different individual machine learning models on the same data. \n",
    "\n",
    "We'll train a `linear regression`, a `decision tree`, and an `SVR` (i.e. [support vector machine regression](https://www.saedsayad.com/support_vector_machine_reg.htm) – *we'll explore this model in greater detail in later tutorials*)  model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and data\n",
    "\n",
    "First let's import libraries and load some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a version of [Kaggle's](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) house price regression data for predicting the price of a house given a set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/Explore-AI/Public-Data/blob/master/house_price_by_area.csv?raw=true\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of two variables:\n",
    " * `LotArea` in m$^{2}$.\n",
    " * `SalePrice` in Rands.\n",
    " \n",
    "We will be using the value of `LotArea` (independent variable) to try and predict the `SalePrice` (dependent variable).   \n",
    "\n",
    "Let's take a look at the data using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"LotArea\"] # Independent variable\n",
    "y = df[\"SalePrice\"] # Dependent variable\n",
    "\n",
    "plt.scatter(X,y) # Create scatter plot\n",
    "plt.title(\"House Price vs Area\")\n",
    "plt.xlabel(\"Lot Area in m$^2$\")\n",
    "plt.ylabel(\"Sale Price in Rands\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "\n",
    "Next we scale and split our dataset into training and testing sets so that we can later evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the scalers\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Normalise X and y\n",
    "X_scaled = x_scaler.fit_transform(np.array(X)[:,np.newaxis]) \n",
    "y_scaled = y_scaler.fit_transform(np.array(y)[:,np.newaxis]) \n",
    "\n",
    "# Set test size to 20 % of training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled,y_scaled,test_size=0.2,random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Linear regression\n",
    "\n",
    "Create and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance using the root mean square error (RMSE) metric: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin_reg.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# Plot the linear regression prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(lin_reg.predict(x_domain))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Decision tree\n",
    "\n",
    "Create and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate regression tree model\n",
    "regr_tree = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "regr_tree.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance using the root mean square error (RMSE) metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr_tree.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# Plot the regression tree prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(regr_tree.predict(x_domain).reshape(-1, 1))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Support vector regression\n",
    "\n",
    "Create and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate support vector regression model\n",
    "sv_reg = SVR(kernel='rbf', gamma='auto')\n",
    "\n",
    "sv_reg.fit(x_train,y_train[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance using the root mean square error (RMSE) metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sv_reg.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# Plot the SVR prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(sv_reg.predict(x_domain).reshape(-1, 1))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Support Vector Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Heterogeneous ensembling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Voting\n",
    "\n",
    "Voting involves combining individual model outputs through a kind of \"[majority rule](https://en.wikipedia.org/wiki/Majority_rule)\" paradigm. \n",
    "\n",
    "This process of prediction aggregation varies in the context of regression and classification tasks:\n",
    "- **Regression** – we take the **average** or **weighted average** of all predictions.\n",
    "- **Classification** – we use the **mode** of all predictions (i.e. most frequently occurring prediction).\n",
    "\n",
    "Within the code below, we instantiate, train, and evaluate such a voting ensemble using `sklearn`'s `VotingRegressor` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the VotingRegressor class from sklearn\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the voting ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models which we'll include in our ensemble. \n",
    "# We pass a list of tuples, which each have a string identifier for the\n",
    "# model (arbitrary choice), along with the actual instantiated sklearn model.  \n",
    "models = [(\"LR\",lin_reg),(\"DT\",regr_tree),(\"SVR\",sv_reg)]\n",
    "\n",
    "# Specify weights for weighted model averaging\n",
    "model_weightings = np.array([0.1,0.3,0.6])\n",
    "v_reg = VotingRegressor(estimators=models,weights=model_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have **built a voting-based ensemble** that combines:\n",
    "- 10% of linear regression model output\n",
    "- 30% of decision tree output\n",
    "- 60% of support vector regression output\n",
    "\n",
    "to make a final prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the voting ensemble:**\n",
    "\n",
    "Note (from the following) that sklearn saves all the input model hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_reg.fit(x_train,y_train[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the performance of the voting ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = v_reg.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# plot the voting regression prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(v_reg.predict(x_domain).reshape(-1, 1))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Voting Ensemble Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it. We managed to combine our initial three models in a way that improves overall performance.\n",
    "\n",
    "> Within the above code we've provided weighted parameters for our voting ensemble which improve the overall performance of the regression task. There **may be better weightings** of these predictions that improve the performance even further. Can you try and run the above code cells now with **your own chosen weights** to see if you can beat the current score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Stacking\n",
    "\n",
    "While we hope that you enjoyed playing around with the voting weights in the previous section, you may have found the overall exercise frustrating due to the difficulty of knowing how your choice of weightings would affect the ensemble's performance. As such, a natural question may have crossed your intelligent mind:\n",
    "\n",
    ">What is a principled way of choosing the optimal voting weights for this ensemble?\n",
    "\n",
    "If we take a step back, we can see that this question can actually be framed as another machine learning problem! After all, this entire time we have been looking at models that can accurately weight various variables in order to map them to an appropriate output. \n",
    "\n",
    "So what stops us from weighting the outputs of multiple base models with that of another model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical approach we are alluding to above, where the outputs of multiple trained base models (sometimes called **base learners**) are fed into another model (referred to as a **meta-model** or meta-learner), is formally known as **stacking**.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/Model_stacking.png?raw=true\" width=65%/>\n",
    "\n",
    "We illustrate this process within the figure above, where we can see the **predictions of base learners feeding into a meta-learner**. The training process is as follows (all handled by the stacking module):\n",
    "\n",
    "   1. Each individual base learner is trained in turn on the same training dataset.\n",
    "    \n",
    "   2. A validation set is used to generate predictions on unseen observations from each of the trained base learners.\n",
    "    \n",
    "   3. These predictions – the outputs from the base learners – are used as the inputs for the meta-learner model. The meta-learner trains on these observations, using the labels from the validation set as the response variable.\n",
    "\n",
    "<a title=\"Supun Setunga / CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0)\"><img width=\"640\" alt=\"Stacking\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/de/Stacking.png\"></a>\n",
    "\n",
    "Training the meta-learner effectively teaches it how to 'blend' the outputs of each of the base learners together to produce a single prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works in `sklearn`. To do so, we'll make use of the `StackingRegressor` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: The StackingRegressor is available in version 0.22 of sklearn onwards. \n",
    "# Please use pip or conda to update your version if this line throws an error. \n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the stacking ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we declare our model list again here \n",
    "models = [(\"LR\",lin_reg),(\"DT\",regr_tree),(\"SVR\",sv_reg)]\n",
    "\n",
    "# Instead of choosing model weightings, we now declare the meta-learner model for our stacking ensemble.  \n",
    "meta_learner_reg = LinearRegression()\n",
    "\n",
    "s_reg = StackingRegressor(estimators=models, final_estimator=meta_learner_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our stacking ensemble declared. It contains our familiar base learners of linear regression, regression tree, and support vector regression. \n",
    "\n",
    "We've also declared our meta-learner as another linear regression model.  \n",
    "\n",
    "> Before we go on to fit and evaluate our model, it's important to understand what we're trading off by using stacking instead of simple voting to possibly improve our ensemble performance. Within the latter technique, once our base models are trained we can immediately get predictions to derive a final weighted prediction. Using stacking, however, we need two training passes: one for the base learners, and then another for the meta-learner. This essentially means that we **gain metric performance** at the expense of **extra computational requirements**.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the stacking ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_reg.fit(x_train,y_train[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the performance of the voting ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = s_reg.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# plot the stacking regression prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(s_reg.predict(x_domain).reshape(-1, 1))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Stacking Ensemble Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it – our stacking ensemble achieves a better RMSE than the voting ensemble; however, it requires an extra training step to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
