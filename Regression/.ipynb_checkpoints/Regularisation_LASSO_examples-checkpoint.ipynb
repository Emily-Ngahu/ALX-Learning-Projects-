{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f87369",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Regularisation – LASSO\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn about L1, L2, LASSO regression, and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "- Understand the difference between L1 and L2 regularisation.\n",
    "- Understand the code required to implement a LASSO regression model.\n",
    "- Understand the concept of sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bad04",
   "metadata": {},
   "source": [
    "## Shrinkage methods\n",
    "\n",
    "In the train on ridge regression, we discovered that it's feasible to enhance the performance of a least squares regression model on the test set by diminishing the size of some of the coefficients, denoted as $\\hat{\\beta}$.\n",
    "\n",
    "This technique employed in ridge regression, aimed at decreasing the magnitude of the coefficients that are deemed less crucial, is known as a _shrinkage_ method. The primary goal here is to \"shrink\" the values of these coefficients to make the model more robust and potentially improve its predictive accuracy.\n",
    "\n",
    "In the context of ridge regression, it is indeed possible to significantly reduce a coefficient's value, pushing it towards zero. However, it's important to note that while these values can get very close to zero, they do not actually reach a zero value. This characteristic means that while ridge regression effectively minimises the influence of less important variables, it does not entirely eliminate them, maintaining all the variables in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dba616",
   "metadata": {},
   "source": [
    "## L1 vs. L2 regularisation\n",
    "\n",
    "Let's recall the optimisation expression for ridge regression:\n",
    "\n",
    "$$\\min_{\\beta} (\\text{RSS} + \\alpha\\sum_{j=1}^p\\beta_j^2)$$\n",
    "\n",
    "Here, the goal is to minimise the residual sum of squares (RSS) alongside a penalty term. This expression can be simplified to:\n",
    "\n",
    "$$\\min_{\\beta} (\\text{RSS} + \\alpha(L2\\_norm))$$\n",
    "\n",
    "where $L2\\_norm$ represents the sum of the squares of the coefficients, emphasising the squared magnitude of coefficients as the penalty.\n",
    "\n",
    "In contrast, LASSO regularisation introduces the $L1\\_norm$, which is the sum of the _absolute_ values of the coefficients:\n",
    "\n",
    "$$\\min_{\\beta} (\\text{RSS} + \\alpha\\sum_{j=1}^p|\\beta_j|)$$\n",
    "\n",
    "or,\n",
    "\n",
    "$$\\min_{\\beta} (\\text{RSS} + \\alpha(L1\\_norm))$$\n",
    "\n",
    "This adaptation, known as LASSO (Least Absolute Shrinkage and Selection Operator), has the distinct ability to reduce some coefficients exactly to zero, thereby effectively eliminating those predictors from the model. This results in a sparse model that utilises a subset of all available predictors, achieving both coefficient shrinkage and feature selection simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3280cf6",
   "metadata": {},
   "source": [
    "## LASSO regression in `sklearn`\n",
    "\n",
    "We'll make use of the same dataset that we did in the least squares and ridge regression trains, such that we can effectively compare the results of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51cee0",
   "metadata": {},
   "source": [
    "**Next** we will load our data as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa789fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983097b6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We'll be using the same dataset and train/test split as in previous regression trains, and again, we will need to standardise the data before it can be used in our LASSO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the response\n",
    "X = df.drop('BiodiversityHealthIndex', axis=1)\n",
    "y = df['BiodiversityHealthIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardization object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef63ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test split module\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25727839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LASSO module\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c39ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LASSO model\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract intercept from model\n",
    "intercept = float(lasso.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficient from model\n",
    "coeff = pd.DataFrame(lasso.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe09175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract intercept\n",
    "print(\"Intercept:\", float(intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb3fd2",
   "metadata": {},
   "source": [
    "Let's take a look at the actual coefficient values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62341cd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Interpretation of the intercept and coefficients\n",
    "\n",
    "The interpretation of the intercept and coefficients remains consistent:\n",
    "\n",
    "- The intercept is understood as the **expected `BiodiversityHealthIndex` when all predictors are at their mean values**.\n",
    "- Each coefficient represents the expected change in the `BiodiversityHealthIndex` for a one-unit increase in the **scaled predictor**.\n",
    "\n",
    "The list of coefficients demonstrates that some coefficients have been effectively reduced to zero, highlighting the impact of regularisation in feature selection and model simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d551d3",
   "metadata": {},
   "source": [
    "## Assessment of predictive accuracy\n",
    "Again, we'll make use of the `sklearn.metrics` library to assess the accuracy of our model. We'll fit the following models as well, in order to compare the LASSO results thoroughly:\n",
    "\n",
    "- A least squares model using all available predictors.\n",
    "- A least squares model using the predictors with non-zero coefficients from LASSO.\n",
    "- A ridge regression model using all available predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic linear model\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Assuming 'df' is the DataFrame loaded from the SDG_15_Life_on_Land_Dataset.csv\n",
    "X_subset = df.drop(['BiodiversityHealthIndex'], axis=1)  # Dropping the response variable\n",
    "\n",
    "# Assuming 'scaler' is already defined and 'y' contains 'BiodiversityHealthIndex'\n",
    "X_subset_scaled = scaler.fit_transform(X_subset)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_subset_scaled, \n",
    "                                                        y, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=1,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "# Least squares using non-zero variables from LASSO\n",
    "lm_subset = LinearRegression()\n",
    "\n",
    "# Least squares using all predictors\n",
    "lm_all = LinearRegression()\n",
    "\n",
    "# Ridge using all predictors\n",
    "ridge = Ridge()\n",
    "\n",
    "lm_subset.fit(X_train2, y_train2)\n",
    "lm_all.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ebee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training set predictions for each model\n",
    "train_lm_subset = lm_subset.predict(X_train2)\n",
    "train_lm_all = lm_all.predict(X_train)\n",
    "train_ridge = ridge.predict(X_train)\n",
    "train_lasso = lasso.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd913535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test set predictions for each model\n",
    "test_lm_subset = lm_subset.predict(X_test2)\n",
    "test_lm_all = lm_all.predict(X_test)\n",
    "test_ridge = ridge.predict(X_test)\n",
    "test_lasso = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"Least Squares, Subset\": metrics.mean_squared_error(y_train2, train_lm_subset),\n",
    "                        \"Least Squares, All\": metrics.mean_squared_error(y_train, train_lm_all),\n",
    "                        \"Ridge\": metrics.mean_squared_error(y_train, train_ridge),\n",
    "                        \"LASSO\": metrics.mean_squared_error(y_train, train_lasso)\n",
    "                    },\n",
    "                    'Test MSE':\n",
    "                    {\n",
    "                        \"Least Squares, Subset\": metrics.mean_squared_error(y_test2, test_lm_subset),\n",
    "                        \"Least Squares, All\": metrics.mean_squared_error(y_test, test_lm_all),\n",
    "                        \"Ridge\": metrics.mean_squared_error(y_test, test_ridge),\n",
    "                        \"LASSO\": metrics.mean_squared_error(y_test, test_lasso)\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from dictionary\n",
    "results_df = pd.DataFrame(data=results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to plot the train and test response variables as a continuous line\n",
    "train_plot = pd.concat([y_train, pd.Series(y_test.iloc[0], index=['Next Time Point'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc37d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(y_test)), lasso.predict(X_test), label='LASSO',color='red')\n",
    "plt.plot(np.arange(len(y_test)), ridge.predict(X_test), label='Ridge', color='yellow')\n",
    "plt.plot(np.arange(len(y_test)), lm_all.predict(X_test), label='Least Squares', color='grey')\n",
    "plt.plot(np.arange(len(y_test)), y_test.values, label='Testing', color='violet')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19fbf1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we have seen or been introduced to:\n",
    "\n",
    "- The purpose of shrinkage methods.\n",
    "- L1 and L2 regularisation.\n",
    "- The LASSO regularisation method.\n",
    "- Comparing LASSO to least squares and ridge regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78859c4b",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "Links to additional resources to help with the understanding of concepts presented in the train:\n",
    "\n",
    "- [Explanation of how LASSO performs variable selection by authors of ISLR](https://www.youtube.com/watch?v=FlSQgXv7Dvw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
