{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Homogeneous ensemble methods\n",
    " \n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we'll learn about homogeneous ensemble methods, employing identical models but varying data or parameters to boost diversity and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "* Understand homogeneous ensemble methods and their setup.\n",
    "* Discover methods to foster diversity in similar model ensembles.\n",
    "* Assess how these methods affect model precision and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneous ensembles\n",
    "\n",
    "Ensemble learning in machine learning is the practice of **combining multiple models** to try and achieve higher overall model performance.\n",
    "\n",
    "A commonly used example of an ensemble model is a random forest. Random forests **combine multiple homogeneous models** (i.e. decision trees) to make predictions. In homogeneous ensemble methods (and random forest models), **diversity in model performance** is promoted by their ability to **control** the **number of predictors** or **portion of data** supplied to each model in the ensemble.\n",
    "\n",
    "Other than the possibility of improving model performance, there are [lots of other reasons](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2) why we might want to build an ensemble.\n",
    "\n",
    "In this train, we explore some **homogeneous ensemble** methods.\n",
    "\n",
    "Some of the most common methods for combining homogeneous models in this way include **bagging** and **boosting**. We will look at how to implement both methods in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneous ensembling in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a version of [Kaggle's](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) house price regression data for predicting the price of a house given a set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/Explore-AI/Public-Data/blob/master/house_price_by_area.csv?raw=true\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of two variables:\n",
    " * `LotArea` in m$^{2}$.\n",
    " * `SalePrice` in Rands.\n",
    " \n",
    "We will be using the value of `LotArea` (independent variable) to try and predict the `SalePrice` (dependent variable).   \n",
    "\n",
    "Let's take a look at the data using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"LotArea\"] # Independent variable\n",
    "y = df[\"SalePrice\"] # Dependent variable\n",
    "\n",
    "plt.scatter(X,y) # Create scatter plot\n",
    "plt.title(\"House Price vs Area\")\n",
    "plt.xlabel(\"Lot Area in m$^2$\")\n",
    "plt.ylabel(\"Sale Price in Rands\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "\n",
    "Next we scale and split our dataset into training and testing sets so that we can later evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the scalers\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Normalise X and y\n",
    "X_scaled = x_scaler.fit_transform(np.array(X)[:,np.newaxis]) \n",
    "y_scaled = y_scaler.fit_transform(np.array(y)[:,np.newaxis]) \n",
    "\n",
    "# Set test size to 20 % of training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled,y_scaled,test_size=0.2,random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Bagging (AKA bootstrap aggregating)\n",
    "\n",
    "Bagging involves training the models of the ensemble on **different subsets of the training data**, particularly on subsets which are **sampled with replacement** from the training data. \n",
    "\n",
    "As such, the resulting 'bag' of models are together more stable due to decreased variance error. \n",
    "\n",
    "The predictions are made by **aggregating the predictions** of all the models in the bag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the code below, we implement such a bagging ensemble using `sklearn`'s `BaggingRegressor` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the BaggingRegressor class from sklearn\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the bagging ensemble:**\n",
    "\n",
    "Here, we pick what base model to use in our ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate decision tree regression model to use as the base model\n",
    "d_tree = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "# Instantiate BaggingRegressor model with a decision tree as the base model\n",
    "bag_reg = BaggingRegressor(estimator = d_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the bagging ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_reg.fit(x_train,y_train[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the performance of the bagging ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag_reg.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# plot the bagging regression prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(bag_reg.predict(x_domain).reshape(-1, 1))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Decision Tree Bagging Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try and run the above code cells a couple of times, we observe that the **RMSE error metric changes**. This occurs due to the random sampling process that is used to build our ensemble. \n",
    "\n",
    "As such, it's often good practice to manually set or record the `random_state` variable used to derive a specific result. This allows your work to be reproducible and easy to share with others.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Boosting\n",
    "\n",
    "Boosting ensembles are built by **combining multiple 'weak models'** (for example, a decision tree with a depth of 1) in a **sequential fashion**. \n",
    "\n",
    "Each model is built from the previous model's training set. This training set is selected through weighted sampling (with replacement) where the weights are the previous model's prediction errors (for example, residuals) on individual training samples.\n",
    "\n",
    "The final predictions are then made by **aggregating the individual predictions** of all the models in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this is implemented in `sklearn` using the `AdaBoostRegressor` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the AdaBoostRegressor class from sklearn\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the boosting ensemble:**\n",
    "\n",
    "Again, we pick what base model to use in our ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate decision tree regression model to use as the base model\n",
    "d_tree = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Instantiate AdaBoostRegressor model with a decision tree as the base model\n",
    "bst_reg = AdaBoostRegressor(estimator = d_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the boosting ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_reg.fit(x_train,y_train[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the performance of the boosting ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = bst_reg.predict(x_test)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "# plot the boosting regression prediction line over data\n",
    "x_domain = np.linspace(min(x_train),max(x_train),100)\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(bag_reg.predict(x_domain).reshape(-1, 1))\n",
    "x_rescaled = x_scaler.inverse_transform(x_domain) \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_rescaled, y_pred_rescaled, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"LotArea in m$^2$\")\n",
    "plt.ylabel(\"SalePrice in Rands\")\n",
    "plt.title(\"Decision Tree Boosting Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
