{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee-v8I_P8rfp"
   },
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>\n",
    "\n",
    "# Exercise: The random forest\n",
    "Â© ExploreAI Academy\n",
    "\n",
    "In this exercise, we build, evaluate and compare random forest regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITqrvAqq8xSC"
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "* Build a random forest regression model in Python.\n",
    "* Experiment with different number of trees.\n",
    "* Evaluate feature importance using a random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this exercise, we will be using the `Crop_yield` dataset that contains various factors that could influence the yield of a particular crop across different regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Fertilizer_Usage</th>\n",
       "      <th>Pesticide_Usage</th>\n",
       "      <th>Irrigation</th>\n",
       "      <th>Crop_Variety</th>\n",
       "      <th>Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East</td>\n",
       "      <td>23.152156</td>\n",
       "      <td>803.362573</td>\n",
       "      <td>Clayey</td>\n",
       "      <td>204.792011</td>\n",
       "      <td>20.767590</td>\n",
       "      <td>1</td>\n",
       "      <td>Variety B</td>\n",
       "      <td>40.316318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>West</td>\n",
       "      <td>19.382419</td>\n",
       "      <td>571.567670</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>256.201737</td>\n",
       "      <td>49.290242</td>\n",
       "      <td>0</td>\n",
       "      <td>Variety A</td>\n",
       "      <td>26.846639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North</td>\n",
       "      <td>27.895890</td>\n",
       "      <td>-8.699637</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>222.202626</td>\n",
       "      <td>25.316121</td>\n",
       "      <td>0</td>\n",
       "      <td>Variety C</td>\n",
       "      <td>-0.323558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East</td>\n",
       "      <td>26.741361</td>\n",
       "      <td>897.426194</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>187.984090</td>\n",
       "      <td>17.115362</td>\n",
       "      <td>0</td>\n",
       "      <td>Variety C</td>\n",
       "      <td>45.440871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East</td>\n",
       "      <td>19.090286</td>\n",
       "      <td>649.384694</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>110.459549</td>\n",
       "      <td>24.068804</td>\n",
       "      <td>1</td>\n",
       "      <td>Variety B</td>\n",
       "      <td>35.478118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Region  Temperature    Rainfall Soil_Type  Fertilizer_Usage  \\\n",
       "0   East    23.152156  803.362573    Clayey        204.792011   \n",
       "1   West    19.382419  571.567670     Sandy        256.201737   \n",
       "2  North    27.895890   -8.699637     Loamy        222.202626   \n",
       "3   East    26.741361  897.426194     Loamy        187.984090   \n",
       "4   East    19.090286  649.384694     Loamy        110.459549   \n",
       "\n",
       "   Pesticide_Usage  Irrigation Crop_Variety      Yield  \n",
       "0        20.767590           1    Variety B  40.316318  \n",
       "1        49.290242           0    Variety A  26.846639  \n",
       "2        25.316121           0    Variety C  -0.323558  \n",
       "3        17.115362           0    Variety C  45.440871  \n",
       "4        24.068804           1    Variety B  35.478118  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df= pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/Python/Crop_yield.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "In the code below, we prepare our dataset for modeling by encoding categorical variables to convert them to a numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Variable Encoding for categorical variables\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Create a function named `train_rf_model` to train and evaluate a random forest regression model on the encoded dataset. \n",
    "\n",
    "The function should take in 3 parameters:\n",
    "- DataFrame containing the encoded features\n",
    "- A string containing the name of the target variable\n",
    "- The number of estimators for the random forest \n",
    "\n",
    "It then returns: \n",
    "- The trained model object \n",
    "- The RMSE and R<sup>2</sup> scores of the model's performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_model(df,target_variable,estimator):\n",
    "    # Splitting the dataset into features and target variable\n",
    "    X = df.drop(target_variable, axis=1)  # independent variables \n",
    "    y = df[target_variable]  # Target variable\n",
    "    \n",
    "    #splitting the data into train and test\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X, y, test_size = 0.2,random_state = 42)\n",
    "    \n",
    "    #initialize the RandomForest model \n",
    "    RF = RandomForestRegressor(n_estimators = estimator, random_state =42)\n",
    "    RF.fit(x_train,y_test)\n",
    "    \n",
    "    #predict x_test model performance \n",
    "    y_pred = RF.fit(x_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_pred,y_test))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return rf_model, {'RMSE': rmse, 'R2': r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use the function you have defined in **Exercise 1** to train and evaluate three different random forest regression models with each having the following number of estimators respectively: `50`, `100`, and `200`. Store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [800, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train and evaluate models with different numbers of estimators\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m estimators_list:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Store the entire returned dictionary as the value for each key\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     model, metric \u001b[38;5;241m=\u001b[39m train_rf_model(df_encoded, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYield\u001b[39m\u001b[38;5;124m'\u001b[39m, n)\n\u001b[0;32m     11\u001b[0m     results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trees\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m     13\u001b[0m results\n",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m, in \u001b[0;36mtrain_rf_model\u001b[1;34m(df, target_variable, estimator)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#initialize the RandomForest model \u001b[39;00m\n\u001b[0;32m     10\u001b[0m RF \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators \u001b[38;5;241m=\u001b[39m estimator, random_state \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m RF\u001b[38;5;241m.\u001b[39mfit(x_train,y_test)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#predict x_test model performance \u001b[39;00m\n\u001b[0;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m RF\u001b[38;5;241m.\u001b[39mfit(x_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    364\u001b[0m     X,\n\u001b[0;32m    365\u001b[0m     y,\n\u001b[0;32m    366\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    367\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    368\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[0;32m    369\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sklearn\\utils\\validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1281\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sql_packages\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [800, 200]"
     ]
    }
   ],
   "source": [
    "# Number of estimators to evaluate\n",
    "estimators_list = [50, 100, 200]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate models with different numbers of estimators\n",
    "for n in estimators_list:\n",
    "    # Store the entire returned dictionary as the value for each key\n",
    "    model, metric = train_rf_model(df_encoded, 'Yield', n)\n",
    "    results[f\"{n} trees\"] = metric\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Say we wish to understand which features have the most impact on crop yield predictions.\n",
    "\n",
    "Use the `feature_importances_` attribute from our lastly trained random forest model in **Exercise 2** to return a series containing the feature importance score for each of the features in our dataset, sorted in descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_model(data, target_variable, n_estimators):\n",
    "\n",
    "    # Splitting the dataset into features and target variable\n",
    "    X = data.drop(target_variable, axis=1)  # Features\n",
    "    y = data[target_variable]  # Target variable\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initializing the RandomForestRegressor with n_estimators\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "\n",
    "    # Training the model on the training set\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Making predictions on the test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)  # Setting squared=False returns the RMSE\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Return the trained model and its performance metrics\n",
    "    return rf_model, {'MSE': mse, 'R2': r2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `train_rf_model` is designed to train and evaluate a random forest regression model. \n",
    "\n",
    "It takes 3 parameters, `data`, `target_variable`, `n_estimators`.\n",
    "\n",
    "The function returns two items: the trained random forest model `rf_model` and a dictionary containing the evaluation metrics, `mse` and `r2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'50 trees': {'MSE': 0.739261264251345, 'R2': 0.9920180175887953},\n",
       " '100 trees': {'MSE': 0.7288864859605081, 'R2': 0.9921300365756436},\n",
       " '200 trees': {'MSE': 0.7200078994393476, 'R2': 0.9922259008186051}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of estimators to evaluate\n",
    "estimators_list = [50, 100, 200]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate models with different numbers of estimators\n",
    "for n in estimators_list:\n",
    "    # Store the entire returned dictionary as the value for each key\n",
    "    model, metric = train_rf_model(df_encoded, 'Yield', n)\n",
    "    results[f\"{n} trees\"] = metric\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we use the previously created function to train and evaluate multiple random forest models, each with a different number of trees (estimators). \n",
    "\n",
    "The for loop iterates over each value in `estimators_list`, where it calls the `train_rf_model()` function, passing the required parameters including the current number of estimators `n` as arguments.\n",
    "\n",
    "The two items returned by the function are stored in separate variables, `model` and `metric`.\n",
    "\n",
    "The `results` dictionary is then used to store the evaluation metrics for each model trained with a different number of trees. The keys are strings indicating the number of trees, and the values are the dictionary of metrics returned by the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rainfall                  0.978910\n",
       "Fertilizer_Usage          0.016670\n",
       "Temperature               0.001971\n",
       "Pesticide_Usage           0.001102\n",
       "Irrigation                0.000251\n",
       "Crop_Variety_Variety B    0.000202\n",
       "Region_West               0.000194\n",
       "Soil_Type_Loamy           0.000161\n",
       "Soil_Type_Sandy           0.000158\n",
       "Crop_Variety_Variety C    0.000143\n",
       "Region_North              0.000120\n",
       "Region_South              0.000118\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract feature importances from the model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Get the names of the features, excluding the target variable 'Yield'\n",
    "feature_names =df_encoded.drop('Yield', axis=1).columns\n",
    "\n",
    "# Create a pandas Series \n",
    "importances = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = importances.sort_values(ascending=False)\n",
    "sorted_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we use the `feature_importances_` attribute of the trained random forest model to extract the importance scores for each feature. \n",
    "\n",
    "The variable `feature_names` stores the list of feature names that were used to train the model. This will be used for mapping each importance score to its corresponding feature name.\n",
    "\n",
    "`importances` is a pandas series object where each feature's importance score is associated with its name. \n",
    "\n",
    "In `sorted_importances`, we get the importances sorted in descending order to get a quick view of the features considered most important by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which top 2 features contribute the most to the model's predictive ability?\n",
    "\n",
    "Understanding feature importance and the contribution of each variable to the model's predictions offers us an opportunity to streamline our models. This understanding enables us to focus on the most influential features, thereby reducing model complexity without significantly sacrificing performance.\n",
    "\n",
    "In refining your model, you should consider an experiment: retrain the model using only the subset of features that have demonstrated the highest importance scores. This encourages an exploration into how much we can reduce complexity while maintaining, or even potentially improving, model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZFCZhaikX+N2/Bg7W6SY+",
   "collapsed_sections": [],
   "name": "Search_algorithms.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "6b5ebbc2c6bde2831bc6c0426f75aca8137ccfc69d329557556ed73faee126ae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
