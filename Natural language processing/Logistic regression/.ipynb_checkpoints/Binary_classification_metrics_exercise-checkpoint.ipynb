{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee-v8I_P8rfp"
   },
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>\n",
    "\n",
    "# Exercise: Binary classification metrics\n",
    "Â© ExploreAI Academy\n",
    "\n",
    "In this exercise, we train a logistic regression model and evaluate its performance by calculating overall accuracy from its confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITqrvAqq8xSC"
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "* Train a logistic regression model\n",
    "* Calculate the model's overall accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and dataset\n",
    "\n",
    "In an effort to conserve a particular endangered animal species, we want to be able to predict the suitability of various habitats. We have a dataset, `habitat_suitability` that contains various environmental and ecological features used to determine whether or not a habitat is suitable for the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "habitat_df= pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/habitat_suitability.csv\")\n",
    "habitat_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Using the dataset, we want to build a classification model that will be able to classify habitats as suitable (1) or unsuitable (0).\n",
    "\n",
    "In the code below, we prepare the dataset and train a logistic regression model, follow these preliminary steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = habitat_df.drop('Habitat Suitability', axis=1)  # Features\n",
    "y = habitat_df['Habitat Suitability']  # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "**a)** \n",
    "\n",
    "Now that we have trained a logistic regression model on the `habitat_suitability` dataset, let's try and get information on how our model performs on new unseen data.\n",
    "\n",
    "Hence: \n",
    "\n",
    "1. Use the trained logistic regression model to make predictions on the test set.\n",
    "2. Import and use `confusion_matrix` from `sklearn.metrics` to generate the confusion matrix for your predictions.\n",
    "3. Display the confusion matrix.\n",
    "\n",
    "**Note:** Remember to scale the test set features to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** \n",
    "\n",
    "The confusion matrix is not easy to read. Let's improve on this by converting it into a dataframe with the following row and column labels `0: Unsuitable`, and `1: Suitable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Now that we can easily interpret our confusion matrix, we want to compare the distribution of the ground truth classifications and the classifications made by the model. \n",
    "\n",
    "That is, we want to find out how many observations were classified as suitable (1) and unsuitable (0) habitats by the model and compare this to the counts originally in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "From the multidimentional array `conf_matrix`, access the true positive, true negative, false positive, and false negative values and store them in the following variables `TP`, `TN`, `FP`, `FN` respectively. \n",
    "\n",
    "Print each value together with their label.\n",
    "\n",
    "**Hint:** Apply your knowledge of where each of these values is located in the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Let's now find out the overall accuracy of our model. \n",
    "\n",
    "**a)** Using the values from **Exercise 3**, calculate the overall accuracy using the formula: \n",
    " \n",
    " $$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions}$$\n",
    "\n",
    "**b)** Comment on the suitability of using accuracy as the sole metric for evaluating the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "**a)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the confusion_matrix function from sklearn's metrics module\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Scale the test dataset features using the same scaler that was applied to the training dataset\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Use the trained logistic regression model to predict the outcomes for the scaled test dataset.\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `confusion_matrix` function from `sklearn` to compare\n",
    "the model's predicted values against the actual values from the test dataset.\n",
    "\n",
    "It displays the correct and incorrect predictions across the different classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the confusion matrix\n",
    "labels = ['0: Unsuitable ', '1: Suitable']\n",
    "\n",
    "# Create a pandas DataFrame from the confusion matrix data and the labels defined above\n",
    "matrix_df = pd.DataFrame(data=conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pandas DataFrame to neatly display our previously generated confusion matrix by labeling the rows and columns according to the outcomes they represent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of each row: Ground truth totals for each class\n",
    "ground_truth_totals = matrix_df.sum(axis=1)\n",
    "print(\"Ground Truth Totals for Each Class:\")\n",
    "print(ground_truth_totals)\n",
    "\n",
    "# Sum of each column: Totals for the predictions for each class\n",
    "prediction_totals = matrix_df.sum(axis=0)\n",
    "print(\"\\nPrediction Totals for Each Class:\")\n",
    "print(prediction_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the ground truth totals for each class by summing up the rows of the confusion matrix DataFrame. \n",
    "\n",
    "On the other hand, we calculate the totals for the predictions for each class by summing up the columns of the confusion matrix DataFrame.\n",
    "\n",
    "By analysing the the ground truth totals can help us in understanding the class balance or imbalance inherent in the dataset while examining the prediction totals can reveal if the model has a bias towards predicting one class more than another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting True Positives (TP) from the confusion matrix, located at index [1, 1]\n",
    "TP = conf_matrix[1, 1]\n",
    "\n",
    "# Extracting True Negatives (TN) from the confusion matrix, located at index [0, 0]\n",
    "TN = conf_matrix[0, 0]\n",
    "\n",
    "# Extracting False Positives (FP) from the confusion matrix, located at index [0, 1]\n",
    "FP = conf_matrix[0, 1]\n",
    "\n",
    "# Extracting False Negatives (FN) from the confusion matrix, located at index [1, 0]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "print(\"True positive:\", TP)\n",
    "print(\"True negative:\", TN)\n",
    "print(\"False positive:\", FP)\n",
    "print(\"False negative\", FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract and calculate each of the confusion matrix components which gives us insights into the types of errors the model is making as well as its successes, which is crucial for understanding how the model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "**a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Overall Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given formula, the correct predictions, which are represented by the `TP` and `TN`, are divided by the total predictions which are all the values: `TP`, `TN`, `FP`, `FN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "From the output showing ground truth totals for each class, we see that the dataset contains significantly more instances labeled as `Unsuitable` (152) compared to `Suitable` (48), highlighting a class imbalance. \n",
    "\n",
    "The high accuracy `87.5%` in this context could therefore be partly due to the model's tendency to predict the majority class showing that we cannot rely on accuracy as the sole metric for evaluating our model's performance.\n",
    "\n",
    "To get a complete picture of the model's performance, you may want to also try other evaluation approaches incorporating precision, recall, F1 score, and ROC-AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZFCZhaikX+N2/Bg7W6SY+",
   "collapsed_sections": [],
   "name": "Search_algorithms.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "6b5ebbc2c6bde2831bc6c0426f75aca8137ccfc69d329557556ed73faee126ae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
