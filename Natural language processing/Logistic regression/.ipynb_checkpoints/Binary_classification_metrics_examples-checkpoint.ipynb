{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Binary classification metrics\n",
    " \n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we'll look at how to assess the performance of a logistic regression model using various binary classification metrics such as accuracy, precision, recall, and the F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "\n",
    "* Understand how to use and interpret the confusion matrix and the classification report to assess the performance of a binary classifier.\n",
    "* Understand the metrics accuracy, precision, recall, and F1-score.\n",
    "* Know how to implement the various binary classification metrics in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a logistic regression model\n",
    "\n",
    "We need to train a model in order to assess its performance.\n",
    "\n",
    "### 1.1 The dataset\n",
    "\n",
    "Let's use a classic binary classification dataset for this task: the `Wisconsin Breast Cancer` dataset. It consists of **569 observations** with **30 predictors** and a single binary response variable.\n",
    "\n",
    "Each observation is the result of a scan on a mass of breast tissue for the purpose of **diagnosing or dismissing breast cancer** in a patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Suppress cell warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **target variable**, which we will store in the DataFrame `y`, consists of two classes, each referring to the diagnosis of a scan of a mass of breast tissue:\n",
    "\n",
    "- **1**: the mass is benign;\n",
    "- **0**: the mass is malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframes\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y, columns=['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the 30 **predictor variables** - all continuous and all encoded to four significant digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's take a look at the **distribution of observations** between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ticks=[0,1], labels=[0,1])\n",
    "plt.ylim(top=300)\n",
    "\n",
    "plt.show()\n",
    "y['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of the **imbalance** here: there are 357 observations in class 1 and only 212 in class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model instance\n",
    "lm = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model intercept\n",
    "lm.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model coefficients\n",
    "coeff_df = pd.DataFrame(lm.coef_.T,X.columns,columns=['Coefficient'])\n",
    "coeff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred_lm = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made some predictions on the test data. Now, let's compare those predictions to the ground truth labels of the test dataset and determine how well the model has actually performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessing model performance using the confusion matrix\n",
    "\n",
    "A confusion matrix, shown below, is a table that describes the performance of a classification model, or classifier, on a set of test data for which the true values are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-conf-matrix.png\" alt=\"sketch-conf-matrix\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the related terminology, each referring to a separate cell in the confusion matrix above. Note that they represent whole numbers and not proportions.\n",
    "\n",
    "- **True negatives (TN)**: These are cases in which we predicted a negative result, and the true result is also negative;\n",
    "- **True positives (TP)**: We predicted a positive result and the true result is positive;\n",
    "- **False positives (FP)**: We predicted a positive result, but the true result is negative. Also known as a **Type I error**;\n",
    "- **False negatives (FN)**: We predicted a negative result, but the true result is positive. Also known as a **Type II error**.\n",
    "\n",
    "Take a moment to familiarise yourself with the table and how we arrive at each of the four categories. Use the sketch below to note which prediction falls into which category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-compare-predictions.png\" alt=\"sketch-compare-predictions\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Confusion matrix in `sklearn`\n",
    "\n",
    "Let's start by importing the `confusion_matrix` object to check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix takes in two arguments: \n",
    "- the unseen test data, `y_test`\n",
    "- our predictions, `pred_lm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not easy to read - let's convert it into a dataframe and add the appropriate labels to make it clear which value is which.\n",
    "\n",
    "The matrix orders the rows and columns in a sorted fashion according to the labels. Our labels are 0 and 1, so the first row/column is 0, and the 2nd row/column is 1. Let's give it the appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0: Malignant', '1: Benign']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, pred_lm), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "That looks better. A few notes on the matrix:\n",
    "\n",
    "- Each **row** represents the **ground truth totals** for _Malignant_ and _Benign_. In other words, the sum of all the values in the first row is the total number of observations in our test dataset labelled _Malignant_.\n",
    "\n",
    "- Each **column** represents the **totals for the predictions** in each of _Malignant_ and _Benign_.\n",
    "\n",
    "- The **intersection** of each row/column gives us a different aspect of the results: **TP**, **TN**, **FP**, or **FN**, as described for the table sketched above.\n",
    "\n",
    "> Therefore, based on the confusion matrix shown here, how many observations did the model classify as `Malignant`, and how many were classified as `Benign`?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Overall accuracy\n",
    "\n",
    "The results shown above lead us to our first classification metric: **overall accuracy**, which we calculate according to the following formula:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{TP + \\space TN}{TP \\space + \\space TN \\space + \\space FP \\space + \\space FN}$$\n",
    "\n",
    "Our overall accuracy is calculated as follows:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{70 + 36}{70 + 36 + 3 + 5} = 0.93$$\n",
    "\n",
    "At first glance this appears to be a useful, catch-all metric which tells us everything we need to know about our model. The problem is that it lacks detail.\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "- We have 100 observations in our test dataset: 90 of them are labelled `No` , the remaining 10 labelled `Yes`. \n",
    "\n",
    "- At prediction time, our model classifies all 100 observations to be in category `No`. Our model made 100 predictions, and got all 90 of the `No` observations correct, giving it an overall accuracy of 90%!\n",
    "\n",
    "- Sounds good right? The problem is that the model got literally none of the `Yes`-labelled observations correct - 0/10! What if the `Yes` cases were for patients who have cancer, or a transaction that is fraudulent? Those are important results, and we would have missed all of them.\n",
    "\n",
    "> That highlights the importance of being accurate not just overall, but in each particular class too.\n",
    "\n",
    "Let's look at a few metrics which are a little more comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assessing model performance using the classification report\n",
    "\n",
    "The **classification report** gives us more information on where our model is going wrong - looking specifically at the performance caused by **Type I & II errors**.  \n",
    "\n",
    "The following **metrics** are calculated as **part of the classification report**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Precision\n",
    "\n",
    "**Answers:** When it predicts `yes`, how often is it correct? \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Recall\n",
    "\n",
    "**Answers:** When the outcome is actually `yes`, how often do we predict it as such?\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 F1 score\n",
    "\n",
    "**Answers:** What is the weighted average of precision and recall? \n",
    "\n",
    "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$\n",
    "\n",
    "F1 score might be a better measure to use if we need to **seek a balance** between Precision and Recall _and_ there is an **uneven class** distribution (large number of 1s vs 0s or vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Classification report in `sklearn`\n",
    "\n",
    "Let's import the `classification_report` object to check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the confusion matrix, the **classification matrix** takes in **two arguments**: the unseen y_test data as well as our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lm, target_names=['0: Malignant', '1: Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "We now have a far more comprehensive view of the performance of our model.\n",
    "\n",
    "- Clearly, the precision, recall and f1-score **values for the benign class are higher**, and this has to do with the class imbalance we referred to earlier in the tutorial. There are more observations with the benign label, so the model gets **_better_** at classifying those ones because it has more evidence of them.\n",
    "\n",
    "- The corresponding **values in the malignant class are lower** for the same reason.\n",
    "\n",
    "- The **weighted f1-score** here gives us a good indication using a single value of how well the model is performed. It is somewhere between the accuracies that the model achieved for each of class 0 and 1, but slightly in favour of class 1, of which there were more examples.\n",
    "\n",
    "- Perhaps the most important information in the above table is in the last row, indicating the weighted average.\n",
    "\n",
    "- Unlike the values in the `macro avg` row which are computed using: \n",
    "\n",
    "$$\\frac{class\\_0\\_metric \\quad + \\quad class\\_1\\_metric}{2}$$ \n",
    "\n",
    "- The `weighted avg` values are computed using: \n",
    "\n",
    "$$\\frac{class\\_0\\_metric \\, \\times \\, \\%\\_class\\_0\\_labels  \\quad + \\quad class\\_1\\_metric \\, \\times \\, \\%\\_class\\_1\\_labels}{2}$$ \n",
    "\n",
    "which takes into account the **proportions of each class** fed into the model (as indicated in the `support` column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we have seen or been introduced to:\n",
    "\n",
    "- Using the confusion matrix to assess the classifications from a binary classifier;\n",
    "- Understanding the four result categories a classification may fall into (TP, TN, etc);\n",
    "- Four metrics for assessing a classifier: accuracy, precision, recall, and F1-score;\n",
    "- The importance of ensuring good performance in _each class_, as opposed to just overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
