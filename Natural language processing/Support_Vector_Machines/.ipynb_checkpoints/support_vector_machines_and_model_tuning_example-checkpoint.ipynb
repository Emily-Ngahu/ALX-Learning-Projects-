{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y31n-PqqqKEx"
   },
   "source": [
    "# Examples: Support vector machines and model tuning\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we will delve into support vector machines (SVMs) and the art of model tuning. We will\n",
    "gain insights into the working principles of SVMs and the role of kernels. By exploring both linear and non-linear SVM classifiers using sklearn, we will discover the significance of hyperplanes and their visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6b5Co3rkes_"
   },
   "source": [
    "## Learning objectives\n",
    "By the end of this train, you should be able to:\n",
    "\n",
    "- Understand the basics of the support vector machine (SVM) model and how it works.\n",
    "- Fit an SVM classifier model using the `sklearn` library and the `linear` and `rbf` kernels.\n",
    "- Understand and visualise the hyperplane of an SVM classifier model. \n",
    "- Tune an SVM model using the `GridSearchCV` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrtI1askqKE1"
   },
   "source": [
    "## Support vector machine\n",
    "\n",
    "How do support vector machines work? \n",
    "\n",
    "The concept itself is quite simple. Let's pretend that we want to classify data points into group $A$ or group $B$. To do this, an SVM will plot our labelled training data as points in space and will look for the widest, clearest gap between points belonging to group A and points belonging to group B. It will then use this newly identified dividing line (known as a hyperplane) and the margin around it to classify new observations. An unseen data point will be classified into group A or B depending on which side of the margin it is closest to. \n",
    "\n",
    "It's easy to do this when our data only has two features. We only need a simple one-dimensional decision boundary (which is basically a line) to classify the data. However, as more features get added the line needs to take on more dimensions because a simple line will no longer work. A line only has one dimension. A flat shape has two dimensions. A 3D shape (unsurprisingly) has three. What do we call something with four or more dimensions? In this case, it's a hyperplane. In SVM, the hyperplane will always have one less dimension ($-1$) than the number of input features ($p$), or a total of $(p-1)$ dimensions.\n",
    "\n",
    "To explain it in more technical terms, the basic idea of the SVM is to separate points using a $(p - 1)$ dimensional hyperplane. What does it mean to separate points? This means that the SVM will construct a decision boundary such that points on the left are assigned a label of $A$ and points on the right are assigned a label of $B$.  When finding this separating hyperplane, we wish to maximise the distance of the nearest points to the hyperplane. The technical term for this is **maximum separating hyperplane**. The data points that dictate where the separating hyperplane goes are called **support vectors**. Have a look at a diagram of the margin, hyperplane, and support vectors below: \n",
    "\n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/support_vector_machine_diagram.jpeg\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Read more about SVMs and how they work [here](https://en.wikipedia.org/wiki/Support_vector_machine).\n",
    "\n",
    "SVMs are interesting models. Like logistic regression, they fit a linear decision boundary. However, unlike logistic regression, SVMs do this in a non-probabilistic way and are able to fit to non-linear data using an algorithm known as the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method). Furthermore, SVMs can be used for both classification and regression. In `sklearn`, these are called `SVC` (support vector classifier) and `SVR` (support vector regression) respectively. \n",
    "\n",
    "One important note: SVC can also refer to support vector **clustering**, so make sure to not get the two confused!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQ7dcOXwqKE4"
   },
   "source": [
    "### Getting started \n",
    "\n",
    "First we import the libraries that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7Jsp07_qKE7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jqXePNzqKFC"
   },
   "source": [
    "### Generate synthetic data\n",
    "\n",
    "Now we'll generate a random dataset to experiment with. We do this by taking a multi-dimensional\n",
    "standard normal distribution and defining classes separated by nested\n",
    "concentric multi-dimensional spheres such that roughly equal numbers of\n",
    "samples are in each class (quantiles of the $\\chi^2$ distribution).\n",
    "\n",
    "The less technical explanation for this is that we've generated a doughnut-shaped dataset, where the samples belonging to one class are generally located in the centre, and the samples belonging to the other class are generally located in the outer ring.\n",
    "\n",
    "**Note:** Don't worry if you don't understand how this works right now. The most important thing is to know that we have a synthetic dataset with a specific structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlSVsnalqKFF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Set the feature dimensionality\n",
    "p = 2\n",
    "\n",
    "# Construct the dataset\n",
    "X, y = make_gaussian_quantiles(cov=3.,\n",
    "                                 n_samples=1000, n_features=p,\n",
    "                                 n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWvHDtO2VOGi"
   },
   "source": [
    "Now let's perform the train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfRv6BpHqKFL"
   },
   "outputs": [],
   "source": [
    "# get training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBb1QtsHqKFP"
   },
   "source": [
    "### Fit an SVM classifier with a linear decision boundary\n",
    "We are going to fit an SVM classifier with a `linear kernel`. This means that we are telling the SVC to fit the data using a linear decision boundary. Let's also take a look at the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "colab_type": "code",
    "id": "npK5t29SqKFQ",
    "outputId": "cb0e86a5-b8f6-413b-f8a5-277376503fdf"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWmm-IHqqKFW"
   },
   "source": [
    "### Plot the decision boundary for the SVC\n",
    "That accuracy score doesn't seem very good. Why do you think that is? To help us understand what's going on, we can use a visualisation.\n",
    "\n",
    "As mentioned before, the SVC calculates and implements a $p-1$ dimensional decision boundary (hyperplane) over the input features. Since we are only looking at two features (our synthetic dataset only has two features, or $p=2$), our hyperplane will only have 1 dimension ($p-1$), so it's going to look like a single line.\n",
    "\n",
    "It should be noted that if our model has more than two features the we can plot the hyperplane for any two features we choose. Below, we'll visualise the hyperplane for the two features in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Feature 1\n",
    "j = 1 # Feature 2\n",
    "\n",
    "svc.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = svc.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Decision Boundary Plot') \n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfGQqzy7qKFd"
   },
   "source": [
    "Do we see the problem here? The doughnut-shaped data are not `linearly separable`, i.e. a linear decision boundary is unable to adequately separate this data. \n",
    "\n",
    "So let's make use of SVM's [kernel trick](https://en.wikipedia.org/wiki/Kernel_method) to use a **non-linear** decision boundary instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-ZI9Z2pqKFf"
   },
   "source": [
    "### Fit an SVC with a non-linear decision boundary\n",
    "\n",
    "For the next section, we're going to use a radial basis function kernel, or `rbf` kernel, which allows the SVC to fit a non-linear decision boundary. You can read more about `rbf` [here](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) and about the effect of this kernel's parameters [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#:~:text=Intuitively%2C%20the%20gamma%20parameter%20defines,the%20model%20as%20support%20vectors.).\n",
    "\n",
    "Let's train a model on our synthetic data using the `rbf` kernel and see if it improves the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "colab_type": "code",
    "id": "VHhjU628qKFi",
    "outputId": "306c8496-769a-4b09-986d-4c18464419f4"
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0Ptuy3Mes6X"
   },
   "source": [
    "As we can see, this is a **much** better result. Let's visualise our decision boundary to see why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEAN9YTyqKFl"
   },
   "source": [
    "### Plot the decision boundary for the SVC using the non-linear `rbf` kernel\n",
    "\n",
    "Once again we will plot the 1-dimensional decision boundary between the two features present in our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "P47cVlDnqKFn",
    "outputId": "05675c93-8395-4eab-b246-c6ada879a0e3"
   },
   "outputs": [],
   "source": [
    "i = 0 # Feature 1\n",
    "j = 1 # Feature 2\n",
    "\n",
    "svc.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = svc.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Decision Boundary Plot - non-linear rbf kernel') \n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdjqJxBufWt_"
   },
   "source": [
    "Clearly the `rbf` kernel is a better fit to our data and results in a better-performing model. If we hadn't changed our parameters and tried a different kernel, we would have thought that our SVC did not perform well on the data and could possibly have discarded it. This highlights the importance of optimising the parameters of a model.\n",
    "\n",
    "Optimising a model is a well-known concept and is known as **model tuning**. Fortunately, `sklearn` provides some tools for us to do this easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpZk-NcfqKFs"
   },
   "source": [
    "## Tuning an SVC model\n",
    "\n",
    "You have probably noticed that the models in the `sklearn` library have quite a few parameters, right? It's possible to use a process called **model tuning** to find the best possible set of parameters for a model. For this purpose, we will use `sklearn`'s [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). This procedure allows us to specify a set of possible parameters for a specific model. `GridSearchCV` will then go through those parameters and try every possible combination of them (kind of like it's working through a grid in a systematic way – that's where the name comes from). `GridSearchCV` will then return the combination of parameters that resulted in a model with the best score. \n",
    "\n",
    "What metric does it use to calculate the score? It will depend on the model being tested or if we specify our own metric. You should check out exactly how this works on your own! We should also note that `GridSearchCV` makes use of **cross-validation**, helping to ensure the robustness of its results.\n",
    "\n",
    "Let's try it out. The first thing to do is to create a dictionary that contains the parameters we want to tune as `keys` and all the different options we want to test for those parameters as `values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bnr4FcIbqKFt"
   },
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), \n",
    "              'C':(0.25,1.0),\n",
    "              'gamma': (1,2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E32ptJpwmtNI"
   },
   "source": [
    "Now, we instantiate an SVC classifier and tell `GridSearchCV` to test it using the parameters we previously specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "k_N5SgT-muv0",
    "outputId": "3d356c34-292b-4ba1-c965-83ded720a1ac"
   },
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "clf = GridSearchCV(svm, parameters)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9CvVFn0ncog"
   },
   "source": [
    "Next, let's make some predictions using our newly tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tMbfWBgmqKFy",
    "outputId": "49cd4a78-3e39-4a1b-e902-57ebc711427e"
   },
   "outputs": [],
   "source": [
    "y_opt = clf.predict(X_test)\n",
    "# Print accuracy scores for tuned and untuned models\n",
    "print(\"Accuracy for untuned model: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy for tuned model: \", accuracy_score(y_test, y_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrBdrvJ6qKF3"
   },
   "source": [
    "\n",
    "In our scenario, the untuned model with a linear kernel achieved a higher accuracy of 98.33% compared to the tuned model with an `rbf` kernel, which had an accuracy of 96.67%. The untuned linear model performed better due to the inherent **simplicity of the linear decision boundary**, which was effective in separating the classes in the dataset. However, despite the lower accuracy, the tuned `rbf` model offers several advantages. Firstly, it has the **flexibility to capture non-linear relationships** in the data, which may result in better generalisation to unseen data in more complex datasets. Additionally, the process of hyperparameter tuning using techniques like GridSearchCV allows for **optimisation of model performance** by finding the best combination of hyperparameters. Even though the untuned model performed well in this case, using a tuned model is essential for maximising model performance, ensuring robustness and improving generalisation to diverse datasets and real-world scenarios.\n",
    "\n",
    "Here are a few things to note about `GridSearchCV`. It takes a base model (in this case, an SVC classifier) and a grid of parameters (our `parameters` dictionary) as input. By default, `GridSearchCV` will continuously refit the model until it finds the model with the best combination of parameters and will return it to the user. Here, we saved the best model as `clf`.\n",
    "\n",
    "We can also get the parameters of the best-performing model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ud8QTMt5qKF5",
    "outputId": "fa3d22bf-ba01-417f-e452-58aad722dbba"
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XOtY9G9GqCy9"
   },
   "source": [
    "**Note**: Depending on the model we are choosing, the number of parameters we are tuning, and the size and complexity of the dataset we are using, `GridSearchCV` can take a lot of time and processing power to perform. Therefore, it's often wise to limit the number and range of the parameters we are testing at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "14_Support Vector Machines and Model Tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
