{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Tree-based models for classification\n",
    " \n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we review tree-based models, specifically decision trees and random forests, examining their application in classification and their implementation using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "\n",
    "* Understand how tree-based models work in the classification setting.\n",
    "* Build decision tree and random forest classification models using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "__Decision trees__ and __random forests__ are models frequently used to solve regression problems. In this train, we will discuss how these tree-based models can be used in classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision trees\n",
    "\n",
    "In this train, we will look at how to build a decision tree classification model. \n",
    "\n",
    "First, let's refresh our memories about decision trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What is a decision tree?\n",
    "\n",
    "A decision tree is a decision support tool that uses a **tree-like graph** or model of decisions and their possible consequences. It is one way to display an algorithm that only contains conditional control statements.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify objects or predict continuous values. You simply ask a series of questions designed to zero in on the classification/prediction. \n",
    "\n",
    "For example, if you wanted to build a **decision tree to classify an animal** you come across while on a hike, you might construct the one shown here:\n",
    "\n",
    "<img src=\"https://cocalc.com/share/raw/8b892baf91f98d0cf6172b872c8ad6694d0f7204/PythonDataScienceHandbook/notebooks/figures/05.08-decision-tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary splitting makes this extremely efficient: In a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes.\n",
    "\n",
    "The trick, of course, comes in deciding **which questions to ask at each step**.\n",
    "\n",
    "In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data. That is, each node in the tree splits the data into two groups using a cut-off value within one of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions made by the tree are the **modes** of the class labels in each specific group of observations (i.e. the training data). This is different from regression, where the predictions were the **means** of the response values in each group.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-classification-tree-mode.png\" alt=\"sketch-classification-tree-mode\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building a decision tree classification model\n",
    "\n",
    "Let's work through an example of how to create a decision tree classifier using `sklearn`. \n",
    "\n",
    "#### Imports\n",
    "\n",
    "Here, we import all the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "In this train, the dataset we will be using is the `Iris dataset`, which is a multivariate dataset where each class refers to a type of Iris plant. This dataset is free and publicly available at the UCI Machine Learning Repository.\n",
    "\n",
    "This dataset contains a set of 150 records with **five attributes**:\n",
    "- Sepal length.\n",
    "- Sepal width.\n",
    "- Petal length.\n",
    "- Petal width.\n",
    "- Species – the type of Iris plant we will be classifying.\n",
    "\n",
    "Let's import the data to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "We will start by preprocessing the data so that we can run it through the model algorithm. This involves:\n",
    "\n",
    "- Splitting the data into features and labels.\n",
    "- Standardising the data using `sklearn`'s `StandardScaler`.\n",
    "- Splitting the data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into features and target\n",
    "y = df['species']\n",
    "X = df.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data\n",
    "standard_scaler = StandardScaler()\n",
    "X_transformed = standard_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.30, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "We will now fit a **decision tree classification** model to our data using `sklearn`'s `DecisionTreeClassifier` with default parameters and a random state of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "Now let's predict the labels for our test set and examine the performance of our model using a **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how many of each class we have in this test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Iris-setosa', 'Iris-versicolor','Iris-virginica']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model does extremely well! Let's also take a look at the **classification report** for our predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Iris-setosa', 'Iris-versicolor','Iris-virginica']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our model does really well, we can use this classification report to gain some insight into how to improve it. \n",
    "\n",
    "We can see here that `Iris-virginica` has the **lowest F1 score**. It is useful to see whether there is a reason for the lower F1 score – a factor to consider is whether the original sample size we trained our model on was truly big enough to expect accurate predictions. If you were the researcher involved in creating this dataset, you might use this insight as a reason to collect more data on `Iris-virginica.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tuning parameters to improve the model\n",
    "\n",
    "For the decision tree algorithm, we can tune parameters to improve the model. The most commonly tuned parameters are:\n",
    "\n",
    "- `max_depth`: the maximum depth of the tree.\n",
    "- `min_samples_leaf`: the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "We encourage you to explore tuning the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Decision trees and overfitting\n",
    "\n",
    "Overfitting is a general property of decision trees. It is very easy to go too deep in the tree and fit details of the particular data rather than the overall properties of the distributions they are drawn from. This issue can be addressed using **random forests**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random forest\n",
    "\n",
    "A random forest is a powerful non-parametric algorithm that is an example of an **ensemble** method built on decision trees, meaning that it relies on aggregating the results of an ensemble of decision trees. \n",
    "\n",
    "The ensemble of trees is randomised and the output is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Evaldas_Vaiciukynas/publication/301638643/figure/fig1/AS:355471899807744@1461762513154/Architecture-of-the-random-forest-model.png\">\n",
    "\n",
    "The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts. That is, a majority vote among a number of estimators can end up being better at predicting outcomes than any of the individual estimators used in the voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building a random forest classification model\n",
    "\n",
    "We will use the same data used in the decision tree classifier above to train a **random forest classifier**. \n",
    "\n",
    "#### Imports\n",
    "\n",
    "First, we need to import `sklearn`'s `RandomForestClassifier`. All other imports needed were declared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "We now fit a random forest classification model to our data using `sklearn`'s `RandomForestClassifier` with default parameters, a **random state** of `42`, and the **number of trees** set to `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "As we did with the decision tree model, let's predict the labels for our test set and examine the performance of our model using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_forest = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Iris-setosa', 'Iris-versicolor','Iris-virginica']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, pred_forest), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the classification report for our predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_forest, target_names=['Iris-setosa', 'Iris-versicolor','Iris-virginica']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tuning parameters to improve the model\n",
    "\n",
    "For the random forest algorithm, we can tune parameters to improve the model. The most commonly tuned parameters are:\n",
    "\n",
    "- `n_estimators`: the number of trees to include in a forest.\n",
    "- `max_depth`: the maximum depth of the tree.\n",
    "- `min_samples_leaf`: the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "We encourage you to explore tuning the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we covered building decision tree and random forest classification models using `sklearn`.\n",
    "\n",
    "The random forest model performs similarly to the decision tree models. This is likely due to the fact that our dataset is small and rather uncomplicated. \n",
    "\n",
    "Practise your model-building skills with other datasets and try to build an intuition for which models work best for different types of tasks/datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
