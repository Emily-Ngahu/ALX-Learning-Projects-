{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Hypothesis testing\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we learn the fundamental principles and techniques of hypothesis testing in statistics. We explore the evaluation of data distributions, the significance of sample means, and the application of various statistical tests to assess hypotheses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "- Understand the fundamentals of hypothesis testing.\n",
    "- Understand and differentiate between Type I and Type II errors.\n",
    "- Know how to test the equality of means for known and unknown population variance.\n",
    "- Know how to test goodness of fit and sample variances using the chi-squared distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG_VUHN5L7lD"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The art of data science lies in understanding the story behind data and what it is telling us. \n",
    "\n",
    "In this train, we will be covering the fundamentals of **hypothesis testing**, an important component in statistical inference – a powerful tool to help us understand our data. In essence, we consider our data to be the result of an experiment which allows us to confirm or reject some initial or [*a priori*](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori) hypothesis we have about a key aspect of the distribution of the underlying data.\n",
    "\n",
    "Typically we refer to this *a priori* hypothesis as the **null hypothesis**, often denoted $H_0$. Quite often we'll see it set up as a description of what would be rather dull if it were true, and in those cases, the real excitement comes from situations where we can reject the null hypothesis. For example, we might have two versions of our website marketing the ExploreAI Academy and set up a null hypothesis that they are equally likely to result in new applicants clicking on the 'Apply Now' button. The fascinating and actionable insight would come from the data confirming that we can reject the null, based on which we would adopt the more successful version and bin the one that wasn't likely to yield results.\n",
    "\n",
    "So how do we accept or reject a hypothesis? First of all, we need to know something about the distribution of what we're testing (in the example above, that might be the difference between the mean click-through rate under 'Web Design A' and that under 'Web Design B', which under our null hypothesis would be zero). Then we use what we know about that distribution to determine how likely it is that our sample data would have been observed if the null hypothesis is true. If that likelihood is sufficiently low, then we conclude that we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfoublnxrG-T"
   },
   "source": [
    "### Hypothesis testing\n",
    "\n",
    "Let's start the illustration of this with an example concerning one observation of a random variable with a known distribution. Suppose we know that the height of data scientists around the globe is normally distributed with a mean of 1.7 metres and a standard deviation of 10 cm. If this is the true distribution, then we can easily answer questions such as: \"How likely is it that a random person drawn from the population of data scientists would be 1.5 m tall, or shorter?\" Let's compute this and plot where on the distribution this point lies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WorSeigrqgs"
   },
   "source": [
    "First, let's import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlGe5eXVL7lA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZl2YTQSrsVt"
   },
   "source": [
    "### Example 1\n",
    "\n",
    "Let's calculate the probability and plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "9R9wUfgfL7lF",
    "outputId": "e451da81-3030-4a50-856b-d76af07f6c94"
   },
   "outputs": [],
   "source": [
    "x = np.arange(1.2,2.2,0.01)    # Range of values for height from 1.2 m to 2.2 m\n",
    "mu = 1.7                       # mu -> distribution mean = 1.7 m\n",
    "sigma = 0.1                    # sigma -> standard deviation = 0.1 m\n",
    "\n",
    "# We now calculate f(x), the probability density function of our normal distribution. \n",
    "f = st.norm.pdf(x, loc = mu, scale = sigma)\n",
    "\n",
    "p = st.norm.cdf(1.5, loc = mu, scale = sigma)\n",
    "print(f'The probability that a random data scientist has a height of 1.5 m is {np.round(p*100,2)} %') # F(1.5), i.e. probability of observing a height <= 1.5 m\n",
    "\n",
    "# Plot the results\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(x,f,'k', label = 'Height normal distribution')\n",
    "plt.axvline(x = 1.5, color = 'r', linestyle = '--', label = '1.5 m Tall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uDJAGucL7lI"
   },
   "source": [
    "We can see that only a relatively small proportion of the probability weight lies below the 1.5 m mark, and our computation tells us that the probability of a random individual from this population being 1.5 m tall or less is about 2.3%. So we would only expect to observe a height as low as this about once in every 44 random draws.\n",
    "\n",
    "With hypothesis testing, we are following a very similar idea, but turning things on their head a little. Here we don't know the population parameters for sure, but are [*hypothesising*](https://en.wikipedia.org/wiki/Hypothesis) what they are. The question we're asking is effectively the reverse of what we did above: Given an observed sample mean, how likely is it that the hypothesised population mean is the true population mean? To answer this question using the normal distribution, we must know what the distribution of the sample mean is, given the null hypothesis. Fortunately, we can calculate the sample mean: $\\bar{X} \\sim N \\left( \\mu, \\frac{\\sigma^2}{n} \\right)$. Here $\\mu$ (mean) and $\\sigma^2$ (variance) are the parameters under the null hypothesis, and $n$ is the size of the sample.\n",
    "\n",
    "To test our null hypothesis, we calculate a **test statistic**: the size of which tells us whether we can accept or reject the null hypothesis. It should be clear that the data can never tell us completely whether or not the hypothesis is true; this is ultimately a probabilistic judgement. Maybe \"Web Design A\" is just as good as \"Web Design B\", but it just so happened that all the people who were going to apply to the ExploreAI Academy ended up seeing the latter rather than the former... but this is unlikely, and that is the point.\n",
    "\n",
    "Statistical inference involves deciding how unlikely we want our data to be before we can reject the null hypothesis. This level of likelihood is known as the **significance level**: if there is less than a 5% chance of having observed our data by chance if the null hypothesis is true, then we say that our outcome is significant at the 5% level (which is a common choice of cut-off in practice, though one should be wary of applying this mindlessly). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PMqmPMSL7lJ"
   },
   "source": [
    "### Example 2\n",
    "\n",
    "Say we conduct a sample among 100 Academy students for their heights and observe a sample mean of 1.682 m. We can go ahead and calculate the test statistic as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nGmK7mEEL7lK",
    "outputId": "ac35426a-b2af-434e-b1e6-c235d41d2c90"
   },
   "outputs": [],
   "source": [
    "st.norm.cdf(1.682, loc = 1.7, scale = (0.1 / 100**0.5)) # Function to create test statistic for our example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cPIlhnLL7lO"
   },
   "source": [
    "The output is 0.0359. This tells us if the null hypothesis holds, there is only a 3.6% probability of observing a sample mean this low with a random sample of 100 students, and we might therefore have reason to doubt about whether the true population mean is as high as 1.7 m. But in order to have a real sense of the likelihood of the null hypothesis being true, we also need to be clear about what alternative hypothesis we're testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NlVdwAbL7lO"
   },
   "source": [
    "## One-sided vs. two-sided hypothesis tests\n",
    "\n",
    "How we frame this alternative hypothesis determines whether our test is one-sided or two-sided. So for the null hypothesis:\n",
    "\n",
    "$$H_0: \\mu = 1.7$$\n",
    "\n",
    "we might have as our alternative hypothesis that the true population mean is less than 1.7 m:\n",
    "\n",
    "$$H_1: \\mu < 1.7,$$\n",
    "\n",
    "or we could allow for the possibility that if it's not truly 1.7 m, it might be higher or lower, in which case we would frame our alternative hypothesis as:\n",
    "\n",
    "$$H_1: \\mu \\ne 1.7.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Suppose that we've chosen a significance level of 5% at which we will reject our null hypothesis. For our one-sided alternative hypothesis, the rejection region is the lower 5% (left tail of the distribution), as in this region there is a more than 95% probability that the true population mean is less than 1.7 m. For the two-sided hypothesis, however, we care about both the lower and upper tails (left and right regions), and given that the normal distribution is symmetric, our rejection region will be the 2.5% tails at both ends of the bell curve. \n",
    "\n",
    "Having a visual might help to make it clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "Kl8noosBL7lP",
    "outputId": "0db83a0d-7241-46c7-a280-26844fe2f26c"
   },
   "outputs": [],
   "source": [
    "x = np.arange(1.67,1.73,0.001)\n",
    "mu = 1.7\n",
    "sigma = 0.1/10 # <-- Standard deviation / squareroot of sample size\n",
    "\n",
    "# Now calculate f(x) - the probability distribution function of a standard normal.\n",
    "f = st.norm.pdf(x, loc = mu, scale = sigma)\n",
    "\n",
    "# Get values for the various significance points\n",
    "r1 = st.norm.ppf(0.05, loc = mu, scale = sigma) # <-- 5% lower tail\n",
    "r2_L = st.norm.ppf(0.025, loc = mu, scale = sigma) # <-- 2.5% lower tail\n",
    "r2_H = st.norm.ppf(0.975, loc = mu, scale = sigma) # <-- 2.5% upper tail\n",
    "\n",
    "# Form the plot.\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(x,f,'k')\n",
    "plt.axvline(x = r1, color = 'b', linestyle = '--', label = 'One-sided hypothesis bound (5%)')\n",
    "plt.axvline(x = r2_L, color = 'r', linestyle = '--', label = 'Lower two-sided hypothesis bound (2.5%)')\n",
    "plt.axvline(x = r2_H, color = 'r', linestyle = '--', label = 'Upper two-sided hypothesis bound (2.5%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iOBzyX5L7lR"
   },
   "source": [
    "Any observed sample mean to the left of the blue line would lead to a rejection of the one-sided hypothesis, while any observed sample mean to the left of the leftmost red line OR to the right of the rightmost would lead to a rejection of the two-sided hypothesis.\n",
    "\n",
    "Let's see how that would play out with our example: We have a $Z$-statistic of -1.8, which is associated with a cumulative distribution function of 0.0359. We can interpret this as follows:\n",
    "\n",
    "- The probability (known as a $p$-value for the one-sided alternative hypothesis) of observing a sample mean as low as 1.682 m, given the null hypothesis, is 0.0359. Since this is less than 0.05, we would reject the null hypothesis at a 5% significance level.\n",
    "\n",
    "- The probability ($p$-value for the two-sided hypothesis) of observing a sample mean at least 1.8 cm away from the hypothesised population mean of 1.7 m is however 0.0359 + 0.0359 = 0.0718 (because of the symmetry of the normal distribution). Since this exceeds 0.05, we cannot reject the null hypothesis at the 5% level.\n",
    "\n",
    "So which type of alternative hypothesis should we use? Well, it depends. If there is a specific reason for a one-sided hypothesis (say we're investigating take-up rates on a direct marketing offer, and we're only concerned about the risk of this being lower than our null hypothesis suggests), then that's what we'll do, but if the intention is merely to investigate whether the null hypothesis is reasonable, then generally a two-sided alternative hypothesis will be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_LtfPhqsL7lS"
   },
   "source": [
    "## False positives and false negatives: Type I and Type II errors\n",
    "\n",
    "In hypothesis testing, there are two types of errors that can be committed. The first is Type I errors also known as False positives and is the likelihood of falsely rejecting a null hypothesis which is in fact true. The second is Type II errors which are known as False negatives which entail accepting (i.e. fail to reject) a null hypothesis which is in fact false. This can be summarised as seen below:\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/hypothesis_testing_Errors.jpg?raw=true\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=50%px/>\n",
    "\n",
    "What happens when we change the significance level? Well, as we bring it down, it becomes harder and harder to reject the null hypothesis. This reduces the risk of false positives and this probability, by definition, is the chosen significance level because the essence of the reasoning we've followed above is that the $p$-value reflects the probability of observing as extreme a sample mean as we have observed by chance if the null hypothesis is true.\n",
    "\n",
    "So reducing that risk may sound like an unambiguously good thing, until you realise that reducing the significance level simultaneously increases the risk of false negatives. There is no neat mapping of significance level onto the risk of committing a Type II error. Choosing the appropriate significance level is therefore a balancing act between the risks of these two types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y37KDShKL7lU"
   },
   "source": [
    "## Testing equality of means: known population variance\n",
    "\n",
    "We've seen above how one would test a sample mean on the basis of a population mean, assuming that the population variance is known. Another situation in which we might have an interest is comparing two samples and testing whether or not the populations from which they're drawn have the same mean.\n",
    "\n",
    "Suppose that we have two sample means which we denote $\\bar{X}$ and $\\bar{Y}$ respectively, drawn from populations with means $\\mu_X$ and $\\mu_Y$ and variances $\\sigma^2_X$ and $\\sigma^2_Y$ respectively:\n",
    "\n",
    "$$\\bar{X} \\sim N \\left( \\mu_X, \\frac{\\sigma^2_X}{n_X} \\right),  \\bar{Y} \\sim N \\left( \\mu_Y, \\frac{\\sigma^2_Y}{n_Y} \\right),$$\n",
    "\n",
    "where $n_X$ and $n_Y$ denote the sizes of the respective samples.\n",
    "\n",
    "Under the null hypothesis we can compute the $Z$-statistic using the following derived equation:\n",
    "\n",
    "$$Z = \\displaystyle \\frac{ \\bar{X}-  \\bar{Y}}{\\sqrt{ \\frac{\\sigma^2_X}{n_X} + \\frac{\\sigma^2_Y}{n_Y}}}.$$\n",
    "\n",
    "The derivation of the equation above is beyond the scope of this train, but the application of it can be used to test the hypothesis in the same way as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XszgZdpL7lW"
   },
   "source": [
    "# More useful distributions for hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xI_2DM-aL7lW"
   },
   "source": [
    "### t-distribution\n",
    "\n",
    "The **t-distribution** also known as the  Student’s t-distribution is a probability distribution that is used to estimate population parameters when the sample size is small and/or when the population variance is unknown.  In 1908, [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset) published the distribution of the $t$-statistic below under a pseudonym, Student, allegedly because his employers (Guinness) were not happy for his identity to be revealed, so we will often hear reference to Student's $t$-distribution. \n",
    "\n",
    "Of interest to Gosset was the question of the distribution of:\n",
    "\n",
    "$$t = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}},$$\n",
    "\n",
    "\n",
    "This distribution has an extra parameter, namely the degrees of freedom: if a random variable $X$ follows the $t$-distribution with $m$ degrees of freedom, we write that $X \\sim t_m.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "\n",
    "Let's have a look at how this distribution, with varying degrees of freedom, compares to the standard normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "gb3wAZgAL7lY",
    "outputId": "95eac66b-7bfb-4d19-fb6b-5e2eff548ef3"
   },
   "outputs": [],
   "source": [
    "x = np.arange(-3,3,0.01)\n",
    "\n",
    "# Calculate f(x) - standard normal\n",
    "f_sn = st.norm.pdf(x)\n",
    "f_t2 = st.t.pdf(x, df = 2)\n",
    "f_t10 = st.t.pdf(x, df = 10)\n",
    "f_t30 = st.t.pdf(x, df = 30)\n",
    "\n",
    "# Plot the results\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(x, f_sn, 'k', label = \"Standard normal\")\n",
    "plt.plot(x, f_t2, '--r', label = \"t with 2 df\")\n",
    "plt.plot(x, f_t10, '--b', label = \"t with 10 df\")\n",
    "plt.plot(x, f_t30, '--g', label = \"t with 30 df\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-Oclz4QL7lb"
   },
   "source": [
    "The plot clearly illustrates that as the degrees of freedom increase, the $t$-distribution gets closer and closer to the standard normal. But with smaller degrees of freedom, the curves are flatter, with greater weight in the tails.\n",
    "\n",
    "So let's dive into degrees of freedom so that we know which $t$-distribution curve we should be using in our hypothesis testing when we don't know the population variance. We can calculate the sample variance:\n",
    "\n",
    "$$s^2_X = \\displaystyle \\frac{1}{n-1} \\sum_{t=1}^n (X_t - \\bar{x})^2.$$\n",
    " \n",
    "The reason that we divide by $n-1$, rather than $n$, is that we lose a **degree of freedom** here because we've had to use the sample mean $\\bar{x}$ as our estimate of $\\mu$: the sample mean together with $n-1$ of the observations determine the value of the $n$th observation, so this is no longer a free parameter. There is only one possible value that $n^{th}$ observation can take, given the other observations and the sample mean. \n",
    "\n",
    "To generalise this rule, a degree of freedom is lost every time we have to estimate a parameter. In the case of the $t$-distribution, we are using the sample variance to estimate the population variance, and a necessary step along the way is using our sample mean to estimate our population mean. Hence we lose a degree of freedom, and our sample variance $s^2_X$ has $n-1$ degrees of freedom for a sample of size $n$. Hence $n-1$ will be the divisor in our computation of sample variance, and $n-1$ will be the number of degrees of freedom of the $t$-distribution we will use for hypothesis testing. In other words, our sample statistic:\n",
    "\n",
    "$$t = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}} \\sim t_{n-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkSNZG7lL7lb"
   },
   "source": [
    "### Example 5\n",
    "\n",
    "So let's go ahead and test an actual hypothesis by repeating the example above. Instead of 10 cm being the population standard deviation, let this be the sample standard deviation $s$ (the square root of the sample variance $s^2$). So our statistic of -1.8 is now distributed $t_{99}$, and we test a one-sided hypothesis as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3vJTwsTsL7ld",
    "outputId": "ec6b3be0-359e-4611-b411-a1108e577162"
   },
   "outputs": [],
   "source": [
    "st.t.cdf(-1.8, df = 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlAnfEsKL7oY"
   },
   "source": [
    "So the $p$-value for the one-sided hypothesis is now 0.0375, which is just higher than the 0.0359 computed under the standard normal distribution with a  known population variance. This is as expected given that we have a reasonable sample of 100 students, but what if we'd observed the same results from a sample of 10? Then our $p$-value would be computed as follows with 9 degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0-ZgbyxkL7oY",
    "outputId": "ed98379d-9285-4c01-bf4b-5ed71b77022b"
   },
   "outputs": [],
   "source": [
    "st.t.cdf(-1.8, df = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE_rKNqhL7ob"
   },
   "source": [
    "Here we would be unable to reject the null hypothesis at the 5% level. The smaller sample means more weight in the tails, implying it becomes harder to reject a given hypothesis with a given sample outcome as the sample gets smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMnlVu31L7oc"
   },
   "source": [
    "## Testing equality of means: unknown population variance\n",
    "\n",
    "We won't go into the mathematical details here (we'll leave that for the super-enthusiastic among you on a rainy Saturday afternoon), but using the $t$-distribution to test whether two means are equal requires making the initial assumption that the two population variances, though unknown, are equal. We then make an estimate for the pooled variance of the two samples as follows:\n",
    "\n",
    "$$s^2 = \\frac{ (n_X-1) s_X^2 + (n_Y - 1) s_Y^2}{n_X + n_Y - 2}.$$\n",
    "\n",
    "Notice the denominator above: We estimate both sample means before estimating the pooled variance, so we lose two degrees of freedom. We then evaluate the following test statistic for the null hypothesis that the means are equal:\n",
    "\n",
    "$$t_{n_X+n_Y-2} = \\frac {\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_X} + \\frac{1}{n_Y}}}.$$\n",
    "\n",
    "Once again the derivation of this is beyond the scope of this train, but the application is exactly as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YstCcPQiL7oe"
   },
   "source": [
    "## Testing equality of variances: F-distribution\n",
    "\n",
    "You may have been troubled by that assumption of equal variances required to test equality of means: What if they're not equal? Maybe we should test that before going down the $t$-test path. The relevant test statistic for this is the $F$-statistic: The $F$-distribution describes the shape of probabilities of the ratio between two sample variances. Continuing with our previous notation, with $s^2_X$ and $s^2_Y$ representing the variances of our two samples, we say that the ratio of these sample variances follows the $F$-distribution with $n_X - 1$ and $n_Y - 1$ degrees of freedom (note: two individual degrees of freedom parameters, as opposed to one for the $t$-distribution), or mathematically:\n",
    "\n",
    "$$\\frac{s_X^2}{s_Y^2} \\sim F_{n_X-1,n_Y-1}.$$\n",
    "\n",
    "### Example 6\n",
    "\n",
    "Let's see what the $F$-distribution looks like for different degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "fGSUbTe8L7og",
    "outputId": "7517ab06-c1d9-4f63-8b02-c025557724bb"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0.01,8,0.01)\n",
    "\n",
    "# Calculate f(x) for F distribution\n",
    "f_1 = st.f.pdf(x, dfn = 5, dfd = 5)\n",
    "f_2 = st.f.pdf(x, dfn = 5, dfd = 10)\n",
    "f_3 = st.f.pdf(x, dfn = 10, dfd = 10)\n",
    "\n",
    "# Plot the results\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(x, f_1, 'k', label = \"F (5,5)\")\n",
    "plt.plot(x, f_2, '--r', label = \"F (5, 10)\")\n",
    "plt.plot(x, f_3, '--b', label = \"F (10, 10)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5_PkGUzL7oj"
   },
   "source": [
    "A few features stand out:\n",
    "\n",
    "* The $F$- distribution is defined only over positive values: since both sample variances are positive, their ratio must also be a positive number. \n",
    "\n",
    "* It is, unlike the normal and $t$-distributions, asymmetric (those of us who've studied a bit of statistics before may recognise it as being positively skewed, with a long tail to the right of the distribution).\n",
    "\n",
    "* The distribution peak is in the region of 1, which is of course the expected value under the null hypothesis of equal variances.\n",
    "\n",
    "Let's perform a test. Suppose that we have samples {$X$} and {$Y$} with the following key statistics:\n",
    "\n",
    "$$n_X = 40, n_Y = 23, s_X^2 = 12.5, s_Y^2 = 20.$$\n",
    "\n",
    "Then our test statistic is 0.625, and we compute a $p$-value as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "38BEB02iL7oj",
    "outputId": "fffb03ab-c42f-4818-d404-8f943267a520"
   },
   "outputs": [],
   "source": [
    "st.f.cdf(12.5 / 20, dfn = 39, dfd = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UkkUKfPL7oo"
   },
   "source": [
    "Clearly we cannot reject the null hypothesis of equal variances at the 5% level, despite the fact that they appear to be so far apart, whether our alternative hypothesis is one-sided or two-sided.\n",
    "\n",
    "It's worth noting that the $t$- and $F$-tests rely on the assumption of normally distributed variables. The central limit theorem guarantees that statistics such as the sample mean and sample variance will be approximately normally distributed for large samples, but for small samples we can go astray if the underlying observations do not themselves follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InjiA7lWL7or"
   },
   "source": [
    "## Testing goodness of fit and sample variances: chi-squared distribution\n",
    "\n",
    "Next we are going to be covering a very important and versatile distribution, the chi-squared ($\\chi^2$) distribution. It has a number of uses, but for the purposes of this train we will focus only on its application in tests of goodness of fit, i.e. how well a given model fits the observed data. \n",
    "\n",
    "### Example 7\n",
    "\n",
    "Imagine we are consulting for a factory that produces dingbats. These dingbats are packaged in groups of 10, and the factory tracks the number of reject dingbats discarded for each successful pack of 10. The factory has shared their data with us, and we want to model the number of rejects before achieving 10 successful dingbats. We hypothesise this follows a negative binomial process with parameters $r = 10$ and $p = 0.8$. This suggests an 80% probability for a dingbat to be non-reject. We'll compare the factory's data for 100 packages, where the `obs` variable records the number of packages with the respective number of rejects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOiTYciQL7ou"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "# Observed data for packages with respective number of rejects\n",
    "obs = np.array([13, 21, 19, 21, 15, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model our expected frequencies, we use the negative binomial distribution for 0 to 5 rejects, and then categorise all 6 or more rejects together. This ensures we follow the guidelines for chi-squared tests that expected frequencies should be 5 or more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # Total number of packages\n",
    "exp = np.array([st.nbinom.pmf(k, 10, 0.8) * n for k in range(6)])\n",
    "exp = np.append(exp, n - np.sum(exp))  # Adjust for \"6 or more\" category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both actual (observed) and expected numbers of packages determined, our next step involves computing a test statistic. This test statistic will compare the observed and expected frequencies across each category of rejects (including our final \"6 or more\" category). By squaring the differences between actual and observed numbers, dividing by the expected number for each category, and summing these values, we obtain a test statistic that approximately follows a $\\chi^2$ distribution with $n-1$ degrees of freedom. Here, $n$ represents the number of categories considered (7 in our case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_stat, p_value = st.chisquare(obs, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation of the test statistic is crucial as it forms the basis for evaluating the goodness of fit between our model and the observed data. The chi-squared distribution provides a reasonable approximation for our purposes, given that all expected frequencies meet the minimum value criterion (generally recommended to be 5 or more).\n",
    "\n",
    "Now, equipped with the null hypothesis that our negative binomial model with $r=10$ and $p=0.8$ accurately describes the underlying data, we proceed to obtain the $p$-value through Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Chi-square statistic: {chi2_stat}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a $p$-value (for instance, if it were 0.37), we assess the possibility of rejecting our null hypothesis. A high $p$-value indicates that, at any standard level of significance, we cannot reject the null hypothesis. Thus, the data suggests our negative binomial model is a good fit for describing the distribution of rejects in the factory's packaging process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqo0sETFL7o4"
   },
   "source": [
    "We can also use the chi-squared test on so-called categorical data, i.e. non-numerical data where observations are effectively classified into one of a number of buckets. For example, suppose that we're interested in the eternally fascinating question of whether left-handed university students are more likely to smoke, and so we've collected the following sample data, presented in the form of a **contingency table**: \n",
    "\n",
    "|              |  Smoker  |  Non-smoker |\n",
    "|--------------|:--------:|:-----------:|\n",
    "| Left-handed  |    24    |       87    |\n",
    "| Right-handed |    111   |      778    |\n",
    "\n",
    "\n",
    "Out of 1,000 students sampled, 111 (11.1%) are left-handed and 135 (13.5%) are smokers. Under the null hypothesis, left-handedness and smoker status are independent, and hence the expected number in each cell can be calculated by multiplying the two relevant probabilities by each other and the total number, as follows:\n",
    "\n",
    "|              |  Smoker  |  Non-smoker |\n",
    "|--------------|:--------:|:-----------:|\n",
    "| Left-handed  | 1000(0.111)(0.135) = 14.985    | 1000(0.111)(0.865) = 96.015  |\n",
    "| Right-handed | 1000(0.889)(0.135) = 120.015   | 1000(0.889)(0.865) = 768.985 |\n",
    "\n",
    "\n",
    "Now we follow once more the procedure of summing the squared differences between observed and expected divided by the expected, which comes to 7.0527. The only question remaining is how many degrees of freedom can be applied to the chi-squared distribution, and for an $m$ by $n$ contingency table, the rule is df $= (m-1)(n-1)$, so for our 2x2 table there is one degree of freedom.\n",
    "\n",
    "Let's  run some code to investigate our null hypothesis of independence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lF20zUMOL7o5",
    "outputId": "4f2807da-8401-40ee-8e8e-cec0b1e4aa30"
   },
   "outputs": [],
   "source": [
    "obs = np.array(([24, 87, 111, 778])).reshape(2,2)\n",
    "\n",
    "st.chi2_contingency(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B86GH_vUL7o8"
   },
   "source": [
    "The output from the above function gives us the test statistic (confirming our 7.0527), the $p$-value, the degrees of freedom (confirming that it's 1) and then an array which replicates our calculation of the expected values under the null hypothesis. The $p$-value of 0.0079 allows us to reject the null hypothesis at the 5% significance level (and even at the 1% level), and we are therefore forced to conclude that there is indeed a relationship between left-handedness and whether or not you smoke. This would be incredibly interesting if the data were not fictitious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPN2-F_drHBv"
   },
   "source": [
    "# Conclusion \n",
    "\n",
    "In this train, we covered some of the technical and statistical concepts of hypothesis testing. We have learned about the types of errors that we  might encounter, how to test the equality of means, and test goodness of fit using chi-squared distribution. \n",
    "\n",
    "You can go ahead and test what you have learned practically to better understand these concepts and how you can apply them to make difficult decisions easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources\n",
    "\n",
    "Links to additional resources to help with the understanding of concepts presented in the train: \n",
    "- [How do t-tests work?](https://statisticsbyjim.com/hypothesis-testing/t-tests-t-values-t-distributions-probabilities/)\n",
    "- [Chi-square goodness of fit test](https://www.statisticssolutions.com/chi-square-goodness-of-fit-test/#:~:text=In%20Chi%2DSquare%20goodness%20of,Poisson)\n",
    "- [Hypothesis testing examples](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hypothesis_testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
